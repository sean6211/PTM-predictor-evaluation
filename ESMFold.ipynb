{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sean6211/PTM-predictor-evaluation/blob/main/ESMFold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ESMFold**\n",
        "for more details see: [Github](https://github.com/facebookresearch/esm/tree/main/esm), [Preprint](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1)\n",
        "\n",
        "#### **Tips and Instructions**\n",
        "- click the little ▶ play icon to the left of each cell below.\n",
        "- use \"/\" to specify chainbreaks, (eg. sequence=\"AAA/AAA\")\n",
        "- for homo-oligomeric predictions, set copies > 1\n",
        "- See [experimental notebook](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/beta/ESMFold_advanced.ipynb) for more advanced options (like sampling).\n",
        "\n",
        "#### **Colab Limitations**\n",
        "- For short monomeric proteins under the length 400, consider using [ESMFold API](https://esmatlas.com/resources?action=fold) (no need for GPU, super fast!)\n",
        "- On Tesla T4 (typical free colab GPU), max total length ~ 900"
      ],
      "metadata": {
        "id": "POQBeXf2Xoxo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boFQEwsNQ4Qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c95f3cf-af63-4945-826b-1e0bf0cda69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing libs...\n",
            "installing openfold...\n",
            "installing esmfold...\n",
            "downloading params...\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "#@title install\n",
        "#@markdown install ESMFold, OpenFold and download Params (~2min 30s)\n",
        "version = \"1\" # @param [\"0\", \"1\"]\n",
        "model_name = \"esmfold_v0.model\" if version == \"0\" else \"esmfold.model\"\n",
        "import os, time\n",
        "if not os.path.isfile(model_name):\n",
        "  # download esmfold params\n",
        "  os.system(\"apt-get install aria2 -qq\")\n",
        "  os.system(f\"aria2c -q -x 16 https://colabfold.steineggerlab.workers.dev/esm/{model_name} &\")\n",
        "\n",
        "  if not os.path.isfile(\"finished_install\"):\n",
        "    # install libs\n",
        "    print(\"installing libs...\")\n",
        "    os.system(\"pip install -q omegaconf pytorch_lightning biopython ml_collections einops py3Dmol modelcif\")\n",
        "    os.system(\"pip install -q git+https://github.com/NVIDIA/dllogger.git\")\n",
        "\n",
        "    print(\"installing openfold...\")\n",
        "    # install openfold\n",
        "    os.system(f\"pip install -q git+https://github.com/sokrypton/openfold.git\")\n",
        "\n",
        "    print(\"installing esmfold...\")\n",
        "    # install esmfold\n",
        "    os.system(f\"pip install -q git+https://github.com/sokrypton/esm.git\")\n",
        "    os.system(\"touch finished_install\")\n",
        "\n",
        "  # wait for Params to finish downloading...\n",
        "  while not os.path.isfile(model_name):\n",
        "    time.sleep(5)\n",
        "  if os.path.isfile(f\"{model_name}.aria2\"):\n",
        "    print(\"downloading params...\")\n",
        "  while os.path.isfile(f\"{model_name}.aria2\"):\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##run **ESMFold**\n",
        "%%time\n",
        "from string import ascii_uppercase, ascii_lowercase\n",
        "import hashlib, re, os\n",
        "import numpy as np\n",
        "import torch\n",
        "from jax.tree_util import tree_map\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import gc\n",
        "\n",
        "def parse_output(output):\n",
        "  pae = (output[\"aligned_confidence_probs\"][0] * np.arange(64)).mean(-1) * 31\n",
        "  plddt = output[\"plddt\"][0,:,1]\n",
        "\n",
        "  bins = np.append(0,np.linspace(2.3125,21.6875,63))\n",
        "  sm_contacts = softmax(output[\"distogram_logits\"],-1)[0]\n",
        "  sm_contacts = sm_contacts[...,bins<8].sum(-1)\n",
        "  xyz = output[\"positions\"][-1,0,:,1]\n",
        "  mask = output[\"atom37_atom_exists\"][0,:,1] == 1\n",
        "  o = {\"pae\":pae[mask,:][:,mask],\n",
        "       \"plddt\":plddt[mask],\n",
        "       \"sm_contacts\":sm_contacts[mask,:][:,mask],\n",
        "       \"xyz\":xyz[mask]}\n",
        "  return o\n",
        "\n",
        "def get_hash(x): return hashlib.sha1(x.encode()).hexdigest()\n",
        "alphabet_list = list(ascii_uppercase+ascii_lowercase)\n",
        "\n",
        "jobname = \"test\" #@param {type:\"string\"}\n",
        "jobname = re.sub(r'\\W+', '', jobname)[:50]\n",
        "\n",
        "sequence = \"GWSTELEKHREELKEFLKKEGITNVEIRIDNGRLEVRVEGGTERLKRFLEELRQKLEKKGYTVDIKIE\" #@param {type:\"string\"}\n",
        "sequence = re.sub(\"[^A-Z:]\", \"\", sequence.replace(\"/\",\":\").upper())\n",
        "sequence = re.sub(\":+\",\":\",sequence)\n",
        "sequence = re.sub(\"^[:]+\",\"\",sequence)\n",
        "sequence = re.sub(\"[:]+$\",\"\",sequence)\n",
        "copies = 1 #@param {type:\"integer\"}\n",
        "if copies == \"\" or copies <= 0: copies = 1\n",
        "sequence = \":\".join([sequence] * copies)\n",
        "num_recycles = 3 #@param [\"0\", \"1\", \"2\", \"3\", \"6\", \"12\", \"24\"] {type:\"raw\"}\n",
        "chain_linker = 25\n",
        "\n",
        "ID = jobname+\"_\"+get_hash(sequence)[:5]\n",
        "seqs = sequence.split(\":\")\n",
        "lengths = [len(s) for s in seqs]\n",
        "length = sum(lengths)\n",
        "print(\"length\",length)\n",
        "\n",
        "u_seqs = list(set(seqs))\n",
        "if len(seqs) == 1: mode = \"mono\"\n",
        "elif len(u_seqs) == 1: mode = \"homo\"\n",
        "else: mode = \"hetero\"\n",
        "\n",
        "if \"model\" not in dir() or model_name != model_name_:\n",
        "  if \"model\" in dir():\n",
        "    # delete old model from memory\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "  model = torch.load(model_name, weights_only=False)\n",
        "  model.eval().cuda().requires_grad_(False)\n",
        "  model_name_ = model_name\n",
        "\n",
        "# optimized for Tesla T4\n",
        "if length > 700:\n",
        "  model.set_chunk_size(64)\n",
        "else:\n",
        "  model.set_chunk_size(128)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "output = model.infer(sequence,\n",
        "                     num_recycles=num_recycles,\n",
        "                     chain_linker=\"X\"*chain_linker,\n",
        "                     residue_index_offset=512)\n",
        "\n",
        "pdb_str = model.output_to_pdb(output)[0]\n",
        "output = tree_map(lambda x: x.cpu().numpy(), output)\n",
        "ptm = output[\"ptm\"][0]\n",
        "plddt = output[\"plddt\"][0,...,1].mean()\n",
        "O = parse_output(output)\n",
        "print(f'ptm: {ptm:.3f} plddt: {plddt:.3f}')\n",
        "os.system(f\"mkdir -p {ID}\")\n",
        "prefix = f\"{ID}/ptm{ptm:.3f}_r{num_recycles}_default\"\n",
        "np.savetxt(f\"{prefix}.pae.txt\",O[\"pae\"],\"%.3f\")\n",
        "with open(f\"{prefix}.pdb\",\"w\") as out:\n",
        "  out.write(pdb_str)"
      ],
      "metadata": {
        "id": "CcyNpAvhTX6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title display (optional) {run: \"auto\"}\n",
        "import py3Dmol\n",
        "pymol_color_list = [\"#33ff33\",\"#00ffff\",\"#ff33cc\",\"#ffff00\",\"#ff9999\",\"#e5e5e5\",\"#7f7fff\",\"#ff7f00\",\n",
        "                    \"#7fff7f\",\"#199999\",\"#ff007f\",\"#ffdd5e\",\"#8c3f99\",\"#b2b2b2\",\"#007fff\",\"#c4b200\",\n",
        "                    \"#8cb266\",\"#00bfbf\",\"#b27f7f\",\"#fcd1a5\",\"#ff7f7f\",\"#ffbfdd\",\"#7fffff\",\"#ffff7f\",\n",
        "                    \"#00ff7f\",\"#337fcc\",\"#d8337f\",\"#bfff3f\",\"#ff7fff\",\"#d8d8ff\",\"#3fffbf\",\"#b78c4c\",\n",
        "                    \"#339933\",\"#66b2b2\",\"#ba8c84\",\"#84bf00\",\"#b24c66\",\"#7f7f7f\",\"#3f3fa5\",\"#a5512b\"]\n",
        "\n",
        "def show_pdb(pdb_str, show_sidechains=False, show_mainchains=False,\n",
        "             color=\"pLDDT\", chains=None, vmin=50, vmax=90,\n",
        "             size=(800,480), hbondCutoff=4.0,\n",
        "             Ls=None,\n",
        "             animate=False):\n",
        "\n",
        "  if chains is None:\n",
        "    chains = 1 if Ls is None else len(Ls)\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js', width=size[0], height=size[1])\n",
        "  if animate:\n",
        "    view.addModelsAsFrames(pdb_str,'pdb',{'hbondCutoff':hbondCutoff})\n",
        "  else:\n",
        "    view.addModel(pdb_str,'pdb',{'hbondCutoff':hbondCutoff})\n",
        "  if color == \"pLDDT\":\n",
        "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':vmin,'max':vmax}}})\n",
        "  elif color == \"rainbow\":\n",
        "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
        "  elif color == \"chain\":\n",
        "    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n",
        "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
        "  if show_sidechains:\n",
        "    BB = ['C','O','N']\n",
        "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
        "                  {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                  {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                  {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "  if show_mainchains:\n",
        "    BB = ['C','O','N','CA']\n",
        "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
        "  view.zoomTo()\n",
        "  if animate: view.animate()\n",
        "  return view\n",
        "\n",
        "color = \"confidence\" #@param [\"confidence\", \"rainbow\", \"chain\"]\n",
        "if color == \"confidence\": color = \"pLDDT\"\n",
        "show_sidechains = False #@param {type:\"boolean\"}\n",
        "show_mainchains = False #@param {type:\"boolean\"}\n",
        "show_pdb(pdb_str, color=color,\n",
        "         show_sidechains=show_sidechains,\n",
        "         show_mainchains=show_mainchains,\n",
        "         Ls=lengths).show()"
      ],
      "metadata": {
        "id": "JM5ciSmeTZKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title plot confidence (optional)\n",
        "\n",
        "dpi = 100 #@param {type:\"integer\"}\n",
        "\n",
        "def plot_ticks(Ls):\n",
        "  Ln = sum(Ls)\n",
        "  L_prev = 0\n",
        "  for L_i in Ls[:-1]:\n",
        "    L = L_prev + L_i\n",
        "    L_prev += L_i\n",
        "    plt.plot([0,Ln],[L,L],color=\"black\")\n",
        "    plt.plot([L,L],[0,Ln],color=\"black\")\n",
        "  ticks = np.cumsum([0]+Ls)\n",
        "  ticks = (ticks[1:] + ticks[:-1])/2\n",
        "  plt.yticks(ticks,alphabet_list[:len(ticks)])\n",
        "\n",
        "def plot_confidence(O, Ls=None, dpi=100):\n",
        "  if \"lm_contacts\" in O:\n",
        "    plt.figure(figsize=(20,4), dpi=dpi)\n",
        "    plt.subplot(1,4,1)\n",
        "  else:\n",
        "    plt.figure(figsize=(15,4), dpi=dpi)\n",
        "    plt.subplot(1,3,1)\n",
        "\n",
        "  plt.title('Predicted lDDT')\n",
        "  plt.plot(O[\"plddt\"])\n",
        "  if Ls is not None:\n",
        "    L_prev = 0\n",
        "    for L_i in Ls[:-1]:\n",
        "      L = L_prev + L_i\n",
        "      L_prev += L_i\n",
        "      plt.plot([L,L],[0,100],color=\"black\")\n",
        "  plt.xlim(0,O[\"plddt\"].shape[0])\n",
        "  plt.ylim(0,100)\n",
        "  plt.ylabel('plDDT')\n",
        "  plt.xlabel('position')\n",
        "  plt.subplot(1,4 if \"lm_contacts\" in O else 3,2)\n",
        "\n",
        "  plt.title('Predicted Aligned Error')\n",
        "  Ln = O[\"pae\"].shape[0]\n",
        "  plt.imshow(O[\"pae\"],cmap=\"bwr\",vmin=0,vmax=30,extent=(0, Ln, Ln, 0))\n",
        "  if Ls is not None and len(Ls) > 1: plot_ticks(Ls)\n",
        "  plt.colorbar()\n",
        "  plt.xlabel('Scored residue')\n",
        "  plt.ylabel('Aligned residue')\n",
        "\n",
        "  if \"lm_contacts\" in O:\n",
        "    plt.subplot(1,4,3)\n",
        "    plt.title(\"contacts from LM\")\n",
        "    plt.imshow(O[\"lm_contacts\"],cmap=\"Greys\",vmin=0,vmax=1,extent=(0, Ln, Ln, 0))\n",
        "    if Ls is not None and len(Ls) > 1: plot_ticks(Ls)\n",
        "    plt.subplot(1,4,4)\n",
        "  else:\n",
        "    plt.subplot(1,3,3)\n",
        "  plt.title(\"contacts from Structure Module\")\n",
        "  plt.imshow(O[\"sm_contacts\"],cmap=\"Greys\",vmin=0,vmax=1,extent=(0, Ln, Ln, 0))\n",
        "  if Ls is not None and len(Ls) > 1: plot_ticks(Ls)\n",
        "  return plt\n",
        "\n",
        "plot_confidence(O, Ls=lengths, dpi=dpi)\n",
        "plt.savefig(f'{prefix}.png',bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HGFBl0QYYQpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download predictions\n",
        "from google.colab import files\n",
        "os.system(f\"zip {ID}.zip {ID}/*\")\n",
        "files.download(f'{ID}.zip')"
      ],
      "metadata": {
        "id": "hkMp_ZwRYfAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Install official ESM + ESMFold and utilities\n",
        "!pip install -q \"fair-esm[esmfold]\" biotite\n",
        "\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tempfile import NamedTemporaryFile\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import biotite.structure.io as bsio\n",
        "import esm\n"
      ],
      "metadata": {
        "id": "c_QQCniJL4NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "#############################################\n",
        "# Load ESM-2 (35M) sequence model\n",
        "#############################################\n",
        "\n",
        "lm_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
        "lm_model = lm_model.to(device)\n",
        "lm_model.eval()  # no backprop, ES is gradient-free\n",
        "\n",
        "AA_LETTERS = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "AA_INDICES = torch.tensor([alphabet.get_idx(a) for a in AA_LETTERS], device=device)\n",
        "\n",
        "mask_idx = alphabet.mask_idx\n",
        "cls_idx  = alphabet.cls_idx\n",
        "eos_idx  = alphabet.eos_idx\n",
        "\n",
        "print(\"Loaded ESM-2 35M model.\")\n",
        "\n",
        "#############################################\n",
        "# Load ESMFold structure model (black-box)\n",
        "#############################################\n",
        "\n",
        "fold_model = esm.pretrained.esmfold_v1()\n",
        "fold_model = fold_model.eval().to(device)\n",
        "fold_model.set_chunk_size(128)  # mildly more memory-efficient on Colab\n",
        "\n",
        "print(\"Loaded ESMFold v1.\")\n"
      ],
      "metadata": {
        "id": "oGuDB7rBMLZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_sequence_from_lm(\n",
        "    model,\n",
        "    alphabet,\n",
        "    length=100,\n",
        "    temperature=1.0,\n",
        "    device=device,\n",
        "):\n",
        "    \"\"\"\n",
        "    Simple left-to-right generation using ESM-2 as a pseudo autoregressive LM.\n",
        "    \"\"\"\n",
        "    total_len = length + 2  # [CLS] + L tokens + [EOS]\n",
        "    tokens = torch.full((1, total_len), fill_value=mask_idx, device=device, dtype=torch.long)\n",
        "    tokens[0, 0] = cls_idx\n",
        "    tokens[0, -1] = eos_idx\n",
        "\n",
        "    for pos in range(1, length + 1):\n",
        "        tokens[0, pos] = mask_idx\n",
        "        out = model(tokens, repr_layers=[], return_contacts=False)\n",
        "        logits = out[\"logits\"][0, pos]  # (vocab_size,)\n",
        "\n",
        "        # Restrict to the 20 standard amino acids\n",
        "        aa_logits = logits[AA_INDICES]\n",
        "        probs = torch.softmax(aa_logits / temperature, dim=-1)\n",
        "\n",
        "        # Sample one amino acid\n",
        "        aa_idx = torch.multinomial(probs, num_samples=1)\n",
        "        tok_id = AA_INDICES[aa_idx]\n",
        "        tokens[0, pos] = tok_id\n",
        "\n",
        "    seq_tokens = tokens[0, 1:-1].tolist()\n",
        "    seq = \"\".join(alphabet.get_tok(t) for t in seq_tokens)\n",
        "    return seq\n",
        "\n",
        "\n",
        "# Quick sanity check\n",
        "test_seq = sample_sequence_from_lm(lm_model, alphabet, length=60, temperature=1.0)\n",
        "print(\"Example generated sequence:\", test_seq)\n",
        "print(\"Length:\", len(test_seq))"
      ],
      "metadata": {
        "id": "0B2Ox5GEMM3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_sequence_with_esmfold(\n",
        "    sequence: str,\n",
        "    fold_model=fold_model,\n",
        "    device=device,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Black-box reward: ESMFold structure prediction + mean pLDDT.\n",
        "    \"\"\"\n",
        "    pdb_str = fold_model.infer_pdb(sequence)\n",
        "\n",
        "    with NamedTemporaryFile(\"w+\", suffix=\".pdb\") as tmp:\n",
        "        tmp.write(pdb_str)\n",
        "        tmp.flush()\n",
        "        struct = bsio.load_structure(tmp.name, extra_fields=[\"b_factor\"])\n",
        "\n",
        "    mean_plddt = float(struct.b_factor.mean())\n",
        "    return mean_plddt\n",
        "\n",
        "\n",
        "def evaluate_model_once(\n",
        "    num_sequences=3,\n",
        "    seq_length=80,\n",
        "    temperature=1.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    For the *current* lm_model parameters, generate sequences,\n",
        "    score each with ESMFold, and return the average score.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for _ in range(num_sequences):\n",
        "        seq = sample_sequence_from_lm(\n",
        "            lm_model,\n",
        "            alphabet,\n",
        "            length=seq_length,\n",
        "            temperature=temperature,\n",
        "            device=device,\n",
        "        )\n",
        "        score = evaluate_sequence_with_esmfold(seq, fold_model=fold_model, device=device)\n",
        "        scores.append(score)\n",
        "    return float(np.mean(scores)), scores"
      ],
      "metadata": {
        "id": "RXwJajD0MOlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EggrollContext:\n",
        "    \"\"\"\n",
        "    Global context used by hooked Linear layers to decide\n",
        "    whether to inject low-rank noise and which noise to use.\n",
        "    \"\"\"\n",
        "    def __init__(self, device):\n",
        "        self.active    = False  # enable/disable noise\n",
        "        self.rank      = 0      # low-rank dimension r\n",
        "        self.sigma     = 0.0    # noise scale\n",
        "        self.thread_id = 0      # population member index\n",
        "        self.device    = device\n",
        "\n",
        "egg_ctx = EggrollContext(device)\n",
        "\n",
        "# Hook all Linear layers in the ESM-2 model\n",
        "lin_modules = []  # (name, module, base_key)\n",
        "\n",
        "for name, module in lm_model.named_modules():\n",
        "    if isinstance(module, nn.Linear):\n",
        "        base_key = random.randint(0, 2**31 - 1)\n",
        "        lin_modules.append((name, module, base_key))\n",
        "        orig_forward = module.forward\n",
        "\n",
        "        def make_forward(m, orig, base_key):\n",
        "            def forward_with_low_rank(x, _orig=orig, _module=m, _base_key=base_key):\n",
        "                # Base linear output\n",
        "                out = _orig(x)\n",
        "                if egg_ctx.active and _module.weight.ndim == 2:\n",
        "                    weight = _module.weight\n",
        "                    out_features, in_features = weight.shape\n",
        "\n",
        "                    # Deterministic RNG for (layer, thread_id)\n",
        "                    g = torch.Generator(device=egg_ctx.device)\n",
        "                    combined_seed = (_base_key ^ egg_ctx.thread_id) & 0x7FFFFFFF\n",
        "                    g.manual_seed(combined_seed)\n",
        "\n",
        "                    r = egg_ctx.rank\n",
        "                    if r > 0:\n",
        "                        perturb = torch.randn(\n",
        "                            in_features + out_features,\n",
        "                            r,\n",
        "                            generator=g,\n",
        "                            device=egg_ctx.device,\n",
        "                        )\n",
        "                        B = perturb[:in_features, :]      # (in, r)\n",
        "                        A = perturb[in_features:, :]      # (out, r)\n",
        "\n",
        "                        # LoRA-style adapter: x @ B -> (batch, r); then @ A^T -> (batch, out)\n",
        "                        adapter = x @ B\n",
        "                        adapter = adapter @ A.t()\n",
        "                        out = out + egg_ctx.sigma * adapter / math.sqrt(r)\n",
        "\n",
        "                return out\n",
        "            return forward_with_low_rank\n",
        "\n",
        "        module.forward = make_forward(module, orig_forward, base_key)\n",
        "\n",
        "print(f\"Hooked {len(lin_modules)} Linear modules for EGGROLL-style noise.\")"
      ],
      "metadata": {
        "id": "LjX5txXYMQCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map parameter names to tensor objects\n",
        "name_to_param = dict(lm_model.named_parameters())\n",
        "\n",
        "# Track only the 2D Linear weight matrices that correspond to hooked modules\n",
        "linear_weights = []  # (param_name, param_tensor, base_key)\n",
        "\n",
        "for name, module, base_key in lin_modules:\n",
        "    w_name = name + \".weight\"\n",
        "    if w_name in name_to_param:\n",
        "        linear_weights.append((w_name, name_to_param[w_name], base_key))\n",
        "\n",
        "print(\"Number of Linear weight matrices to optimize:\", len(linear_weights))"
      ],
      "metadata": {
        "id": "rYY2dBrvMRZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def accumulate_linear_grad_buffers(\n",
        "    grad_buffers,\n",
        "    linear_weights,\n",
        "    rank,\n",
        "    sigma,\n",
        "    thread_id,\n",
        "    device,\n",
        "    weight,\n",
        "):\n",
        "    \"\"\"\n",
        "    Re-generate the low-rank noise for each Linear weight and accumulate\n",
        "    weight * sigma * ΔW into grad_buffers[name].\n",
        "    \"\"\"\n",
        "    for param_name, param, base_key in linear_weights:\n",
        "        out_features, in_features = param.data.shape\n",
        "\n",
        "        g = torch.Generator(device=device)\n",
        "        combined_seed = (base_key ^ thread_id) & 0x7FFFFFFF\n",
        "        g.manual_seed(combined_seed)\n",
        "\n",
        "        perturb = torch.randn(\n",
        "            in_features + out_features,\n",
        "            rank,\n",
        "            generator=g,\n",
        "            device=device,\n",
        "        )\n",
        "        B = perturb[:in_features, :]     # (in, r)\n",
        "        A = perturb[in_features:, :]     # (out, r)\n",
        "        delta = (A @ B.t()) / math.sqrt(rank)  # (out, in)\n",
        "\n",
        "        if param_name not in grad_buffers:\n",
        "            grad_buffers[param_name] = torch.zeros_like(param.data)\n",
        "        grad_buffers[param_name].add_(weight * sigma * delta)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def es_step_eggroll(\n",
        "    lm_model,\n",
        "    linear_weights,\n",
        "    rank,\n",
        "    sigma,\n",
        "    pop_size,\n",
        "    lr,\n",
        "    device,\n",
        "    eval_fn,\n",
        "    num_sequences,\n",
        "    seq_length,\n",
        "    temperature,\n",
        "):\n",
        "    \"\"\"\n",
        "    One EGGROLL-style ES step:\n",
        "      - Evaluate pop_size perturbations via low-rank LoRA-like forward\n",
        "      - Compute normalized rewards\n",
        "      - Reconstruct perturbations to form a full-rank update\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "\n",
        "    # 1) Evaluate each population member\n",
        "    for j in range(pop_size):\n",
        "        egg_ctx.active    = True\n",
        "        egg_ctx.rank      = rank\n",
        "        egg_ctx.sigma     = sigma\n",
        "        egg_ctx.thread_id = j\n",
        "\n",
        "        mean_score, _ = eval_fn(\n",
        "            num_sequences=num_sequences,\n",
        "            seq_length=seq_length,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        rewards.append(mean_score)\n",
        "\n",
        "    # Turn off perturbations afterwards\n",
        "    egg_ctx.active = False\n",
        "\n",
        "    rewards_t = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
        "\n",
        "    # 2) Normalize rewards (zero-mean, unit variance)\n",
        "    if rewards_t.std() > 1e-8:\n",
        "        norm_rewards = (rewards_t - rewards_t.mean()) / (rewards_t.std() + 1e-8)\n",
        "    else:\n",
        "        norm_rewards = rewards_t - rewards_t.mean()\n",
        "\n",
        "    # 3) Accumulate gradient estimate in parameter space\n",
        "    grad_buffers = {}\n",
        "    for j, R in enumerate(norm_rewards):\n",
        "        accumulate_linear_grad_buffers(\n",
        "            grad_buffers,\n",
        "            linear_weights,\n",
        "            rank=rank,\n",
        "            sigma=sigma,\n",
        "            thread_id=j,\n",
        "            device=device,\n",
        "            weight=R.item(),\n",
        "        )\n",
        "\n",
        "    # 4) Apply update: θ_new = θ + lr * grad_estimate\n",
        "    scale = lr / (pop_size * (sigma**2 + 1e-8))\n",
        "    for param_name, param, base_key in linear_weights:\n",
        "        param.data.add_(scale * grad_buffers[param_name])\n",
        "\n",
        "    return float(rewards_t.mean().item()), float(rewards_t.max().item()), rewards"
      ],
      "metadata": {
        "id": "KCdclqrQMSzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "# ES hyperparameters (Colab-friendly)\n",
        "##########################################\n",
        "\n",
        "SEQ_LENGTH      = 60       # amino acids per sequence\n",
        "NUM_SEQUENCES   = 2        # sequences per population member\n",
        "POP_SIZE        = 4        # ES population size\n",
        "RANK            = 4        # low-rank dimension r\n",
        "SIGMA           = 0.05     # noise scale\n",
        "LR              = 0.1      # learning rate in parameter space\n",
        "NUM_STEPS       = 3        # ES iterations (increase if GPU allows)\n",
        "TEMPERATURE     = 1.0      # sampling temperature for LM\n",
        "\n",
        "print(\"=== ES CONFIGURATION ===\")\n",
        "print(f\"SEQ_LENGTH    = {SEQ_LENGTH}\")\n",
        "print(f\"NUM_SEQUENCES = {NUM_SEQUENCES}\")\n",
        "print(f\"POP_SIZE      = {POP_SIZE}\")\n",
        "print(f\"RANK          = {RANK}\")\n",
        "print(f\"SIGMA         = {SIGMA}\")\n",
        "print(f\"LR            = {LR}\")\n",
        "print(f\"NUM_STEPS     = {NUM_STEPS}\")\n",
        "\n",
        "\n",
        "def eval_fn(num_sequences=NUM_SEQUENCES, seq_length=SEQ_LENGTH, temperature=TEMPERATURE):\n",
        "    return evaluate_model_once(\n",
        "        num_sequences=num_sequences,\n",
        "        seq_length=seq_length,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Run ES\n",
        "##########################################\n",
        "\n",
        "history = []\n",
        "\n",
        "for step in range(1, NUM_STEPS + 1):\n",
        "    avg_reward, max_reward, all_rewards = es_step_eggroll(\n",
        "        lm_model,\n",
        "        linear_weights,\n",
        "        rank=RANK,\n",
        "        sigma=SIGMA,\n",
        "        pop_size=POP_SIZE,\n",
        "        lr=LR,\n",
        "        device=device,\n",
        "        eval_fn=eval_fn,\n",
        "        num_sequences=NUM_SEQUENCES,\n",
        "        seq_length=SEQ_LENGTH,\n",
        "        temperature=TEMPERATURE,\n",
        "    )\n",
        "    history.append((avg_reward, max_reward))\n",
        "    print(f\"[Step {step}] avg reward = {avg_reward:.2f}, max reward = {max_reward:.2f}\")"
      ],
      "metadata": {
        "id": "VXFufkCOMUPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sample_and_score_n(n=3, length=SEQ_LENGTH):\n",
        "    seqs = []\n",
        "    scores = []\n",
        "    for _ in range(n):\n",
        "        seq = sample_sequence_from_lm(\n",
        "            lm_model,\n",
        "            alphabet,\n",
        "            length=length,\n",
        "            temperature=TEMPERATURE,\n",
        "            device=device,\n",
        "        )\n",
        "        score = evaluate_sequence_with_esmfold(seq, fold_model=fold_model, device=device)\n",
        "        seqs.append(seq)\n",
        "        scores.append(score)\n",
        "    return seqs, scores\n",
        "\n",
        "\n",
        "print(\"\\nSampling sequences from the (possibly optimized) model...\")\n",
        "final_seqs, final_scores = sample_and_score_n(n=3, length=SEQ_LENGTH)\n",
        "\n",
        "for i, (s, sc) in enumerate(zip(final_seqs, final_scores), 1):\n",
        "    print(f\"\\nSequence {i} (mean pLDDT {sc:.2f}):\\n{s}\")"
      ],
      "metadata": {
        "id": "86IB198TMWSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "\n",
        "##########################################\n",
        "# 1) Measure a proper baseline\n",
        "##########################################\n",
        "\n",
        "# We'll use the same length we train on\n",
        "TRAIN_SEQ_LENGTH = 60\n",
        "\n",
        "baseline_mean, baseline_scores = evaluate_model_once(\n",
        "    num_sequences=10,\n",
        "    seq_length=TRAIN_SEQ_LENGTH,\n",
        "    temperature=1.0,\n",
        ")\n",
        "print(f\"[Baseline] mean pLDDT over 10 sequences: {baseline_mean:.2f}\")\n",
        "print(\"Individual scores:\", [f\"{s:.2f}\" for s in baseline_scores])\n",
        "\n",
        "##########################################\n",
        "# 2) New ES hyperparameters (more stable)\n",
        "##########################################\n",
        "\n",
        "TRAIN_NUM_SEQUENCES = 4   # more sequences per member → less noisy rewards\n",
        "TRAIN_POP_SIZE      = 8   # larger population if GPU allows\n",
        "TRAIN_RANK          = 4   # low-rank dimension (keep small for Colab)\n",
        "TRAIN_SIGMA         = 0.02\n",
        "TRAIN_LR            = 0.03\n",
        "TRAIN_NUM_STEPS     = 20  # more ES iterations\n",
        "TRAIN_TEMPERATURE   = 1.0\n",
        "\n",
        "print(\"\\n=== NEW ES CONFIGURATION ===\")\n",
        "print(f\"TRAIN_SEQ_LENGTH    = {TRAIN_SEQ_LENGTH}\")\n",
        "print(f\"TRAIN_NUM_SEQUENCES = {TRAIN_NUM_SEQUENCES}\")\n",
        "print(f\"TRAIN_POP_SIZE      = {TRAIN_POP_SIZE}\")\n",
        "print(f\"TRAIN_RANK          = {TRAIN_RANK}\")\n",
        "print(f\"TRAIN_SIGMA         = {TRAIN_SIGMA}\")\n",
        "print(f\"TRAIN_LR            = {TRAIN_LR}\")\n",
        "print(f\"TRAIN_NUM_STEPS     = {TRAIN_NUM_STEPS}\")\n",
        "\n",
        "##########################################\n",
        "# 3) Restrict which layers we mutate\n",
        "##########################################\n",
        "# Instead of updating all 74 Linear weights, we only update the last few.\n",
        "# This often stabilizes training and keeps the model closer to its pretrained prior.\n",
        "\n",
        "NUM_LINEAR_TO_TRAIN = 24  # you can change this (e.g., 16, 32, etc.)\n",
        "\n",
        "if len(linear_weights) <= NUM_LINEAR_TO_TRAIN:\n",
        "    trained_linear_weights = linear_weights\n",
        "    print(f\"\\nTraining ALL {len(trained_linear_weights)} Linear weights.\")\n",
        "else:\n",
        "    trained_linear_weights = linear_weights[-NUM_LINEAR_TO_TRAIN:]\n",
        "    print(f\"\\nTraining ONLY the last {NUM_LINEAR_TO_TRAIN} Linear weights out of {len(linear_weights)} total.\")\n",
        "\n",
        "# Small helper so es_step_eggroll can call evaluate_model_once with named args\n",
        "def train_eval_fn(num_sequences, seq_length, temperature):\n",
        "    return evaluate_model_once(\n",
        "        num_sequences=num_sequences,\n",
        "        seq_length=seq_length,\n",
        "        temperature=temperature,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "UjV8xT1uMX0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = []  # list of (avg_reward, max_reward)\n",
        "\n",
        "print(\"\\n=== Starting ES training ===\")\n",
        "for step in range(1, TRAIN_NUM_STEPS + 1):\n",
        "    avg_reward, max_reward, all_rewards = es_step_eggroll(\n",
        "        lm_model,\n",
        "        trained_linear_weights,      # <--- restricted subset\n",
        "        rank=TRAIN_RANK,\n",
        "        sigma=TRAIN_SIGMA,\n",
        "        pop_size=TRAIN_POP_SIZE,\n",
        "        lr=TRAIN_LR,\n",
        "        device=device,\n",
        "        eval_fn=train_eval_fn,\n",
        "        num_sequences=TRAIN_NUM_SEQUENCES,\n",
        "        seq_length=TRAIN_SEQ_LENGTH,\n",
        "        temperature=TRAIN_TEMPERATURE,\n",
        "    )\n",
        "    history.append((avg_reward, max_reward))\n",
        "    print(\n",
        "        f\"[Step {step:02d}] \"\n",
        "        f\"avg reward = {avg_reward:.2f}, \"\n",
        "        f\"max reward = {max_reward:.2f}, \"\n",
        "        f\"pop rewards = {[f'{r:.2f}' for r in all_rewards]}\"\n",
        "    )\n",
        "\n",
        "##########################################\n",
        "# Post-training evaluation (same as baseline)\n",
        "##########################################\n",
        "post_mean, post_scores = evaluate_model_once(\n",
        "    num_sequences=10,\n",
        "    seq_length=TRAIN_SEQ_LENGTH,\n",
        "    temperature=1.0,\n",
        ")\n",
        "print(\"\\n=== Baseline vs Post-training ===\")\n",
        "print(f\"Baseline mean pLDDT over 10 seqs: {baseline_mean:.2f}\")\n",
        "print(f\"Post-train mean pLDDT over 10 seqs: {post_mean:.2f}\")\n",
        "print(\"Post-train individual scores:\", [f\"{s:.2f}\" for s in post_scores])\n"
      ],
      "metadata": {
        "id": "vA5GR0C8Pm3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "##########################################\n",
        "# Plot avg and max reward vs training step\n",
        "##########################################\n",
        "\n",
        "avg_hist = [a for (a, m) in history]\n",
        "max_hist = [m for (a, m) in history]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(avg_hist, label=\"avg reward\")\n",
        "plt.plot(max_hist, label=\"max reward\")\n",
        "plt.xlabel(\"ES step\")\n",
        "plt.ylabel(\"Reward (mean pLDDT)\")\n",
        "plt.title(\"ES reward during training\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "##########################################\n",
        "# Sample and score a few sequences from the final model\n",
        "##########################################\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_and_score_n(n=3, length=TRAIN_SEQ_LENGTH):\n",
        "    seqs = []\n",
        "    scores = []\n",
        "    for _ in range(n):\n",
        "        seq = sample_sequence_from_lm(\n",
        "            lm_model,\n",
        "            alphabet,\n",
        "            length=length,\n",
        "            temperature=TRAIN_TEMPERATURE,\n",
        "            device=device,\n",
        "        )\n",
        "        score = evaluate_sequence_with_esmfold(seq, fold_model=fold_model, device=device)\n",
        "        seqs.append(seq)\n",
        "        scores.append(score)\n",
        "    return seqs, scores\n",
        "\n",
        "print(\"\\n=== Sampling from final (ES-updated) model ===\")\n",
        "final_seqs, final_scores = sample_and_score_n(n=3, length=TRAIN_SEQ_LENGTH)\n",
        "\n",
        "for i, (s, sc) in enumerate(zip(final_seqs, final_scores), 1):\n",
        "    print(f\"\\nSequence {i} (mean pLDDT {sc:.2f}):\\n{s}\")\n"
      ],
      "metadata": {
        "id": "CMqP_Ns7Pon1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "##########################################\n",
        "# 1. CONFIG (edit these to trade off runtime vs robustness)\n",
        "##########################################\n",
        "\n",
        "N_EXPERIMENTS          = 3      # number of ES runs with different seeds (e.g. 3–5)\n",
        "BASELINE_N_SEQS        = 30     # sequences for baseline evaluation (30–50 recommended)\n",
        "POST_N_SEQS            = 30     # sequences for post-training evaluation\n",
        "EVAL_SEQ_LENGTH        = 60\n",
        "EVAL_TEMPERATURE       = 1.0\n",
        "\n",
        "# ES training hyperparameters PER EXPERIMENT\n",
        "TRAIN_SEQ_LENGTH       = 60\n",
        "TRAIN_NUM_SEQUENCES    = 4      # sequences per population member per step\n",
        "TRAIN_POP_SIZE         = 8      # population size (try 8–16 if GPU allows)\n",
        "TRAIN_RANK             = 4      # low-rank dimension\n",
        "TRAIN_SIGMA            = 0.02\n",
        "TRAIN_LR               = 0.03\n",
        "TRAIN_NUM_STEPS        = 15     # steps per experiment (30 is stronger but slower)\n",
        "TRAIN_TEMPERATURE      = 1.0\n",
        "\n",
        "# Which subset of linear layers to train (last K)\n",
        "NUM_LINEAR_TO_TRAIN    = 24\n",
        "\n",
        "print(\"=== MULTI-RUN CONFIG ===\")\n",
        "print(f\"N_EXPERIMENTS         = {N_EXPERIMENTS}\")\n",
        "print(f\"BASELINE_N_SEQS       = {BASELINE_N_SEQS}\")\n",
        "print(f\"POST_N_SEQS           = {POST_N_SEQS}\")\n",
        "print(f\"TRAIN_POP_SIZE        = {TRAIN_POP_SIZE}\")\n",
        "print(f\"TRAIN_NUM_STEPS       = {TRAIN_NUM_STEPS}\")\n",
        "print(f\"TRAIN_NUM_SEQUENCES   = {TRAIN_NUM_SEQUENCES}\")\n",
        "print(f\"NUM_LINEAR_TO_TRAIN   = {NUM_LINEAR_TO_TRAIN}\")\n",
        "\n",
        "##########################################\n",
        "# 2. Choose subset of linear weights to train\n",
        "##########################################\n",
        "\n",
        "if len(linear_weights) <= NUM_LINEAR_TO_TRAIN:\n",
        "    trained_linear_weights_global = linear_weights\n",
        "    print(f\"\\nTraining ALL {len(trained_linear_weights_global)} Linear weights.\")\n",
        "else:\n",
        "    trained_linear_weights_global = linear_weights[-NUM_LINEAR_TO_TRAIN:]\n",
        "    print(f\"\\nTraining ONLY the last {NUM_LINEAR_TO_TRAIN} Linear weights out of {len(linear_weights)} total.\")\n",
        "\n",
        "##########################################\n",
        "# 3. Save a base copy of the model to reset each experiment\n",
        "##########################################\n",
        "\n",
        "base_state_dict = {k: v.detach().cpu().clone() for k, v in lm_model.state_dict().items()}\n",
        "print(\"\\nSaved base model state for resetting between experiments.\")\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 4. Helper: evaluate model distribution (mean, std, tails)\n",
        "##########################################\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model_distribution(\n",
        "    n_sequences: int,\n",
        "    seq_length: int,\n",
        "    temperature: float,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate n_sequences from the *current* lm_model, score with ESMFold,\n",
        "    and return a dict with mean, std, and tail fractions.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for _ in range(n_sequences):\n",
        "        seq = sample_sequence_from_lm(\n",
        "            lm_model,\n",
        "            alphabet,\n",
        "            length=seq_length,\n",
        "            temperature=temperature,\n",
        "            device=device,\n",
        "        )\n",
        "        score = evaluate_sequence_with_esmfold(seq, fold_model=fold_model, device=device)\n",
        "        scores.append(score)\n",
        "\n",
        "    scores = np.array(scores, dtype=np.float32)\n",
        "    mean_score = float(scores.mean())\n",
        "    std_score  = float(scores.std(ddof=1)) if len(scores) > 1 else 0.0\n",
        "    frac_gt_60 = float((scores > 60.0).mean())\n",
        "    frac_lt_30 = float((scores < 30.0).mean())\n",
        "\n",
        "    metrics = {\n",
        "        \"mean\": mean_score,\n",
        "        \"std\": std_score,\n",
        "        \"frac_gt_60\": frac_gt_60,\n",
        "        \"frac_lt_30\": frac_lt_30,\n",
        "        \"scores\": scores,\n",
        "    }\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "V3xEfW_4Pqjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import random\n",
        "\n",
        "def run_es_experiment(exp_id: int, seed: int):\n",
        "    \"\"\"\n",
        "    Run one ES experiment:\n",
        "      1) Reset model to base_state_dict\n",
        "      2) Evaluate baseline distribution\n",
        "      3) Run ES training\n",
        "      4) Evaluate post-training distribution\n",
        "    Returns a dict with baseline + post metrics and training history.\n",
        "    \"\"\"\n",
        "    print(f\"\\n====================\")\n",
        "    print(f\"Starting experiment {exp_id} with seed {seed}\")\n",
        "    print(f\"====================\")\n",
        "\n",
        "    # --- Set seeds for reproducibility ---\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # --- Reset model to base state ---\n",
        "    lm_model.load_state_dict(base_state_dict)\n",
        "    lm_model.to(device)\n",
        "    lm_model.eval()\n",
        "\n",
        "    # --- Evaluate baseline distribution ---\n",
        "    baseline_metrics = evaluate_model_distribution(\n",
        "        n_sequences=BASELINE_N_SEQS,\n",
        "        seq_length=EVAL_SEQ_LENGTH,\n",
        "        temperature=EVAL_TEMPERATURE,\n",
        "    )\n",
        "    print(\n",
        "        f\"[Exp {exp_id}] Baseline mean pLDDT = {baseline_metrics['mean']:.2f} \"\n",
        "        f\"+/- {baseline_metrics['std']:.2f}, \"\n",
        "        f\"frac>60 = {baseline_metrics['frac_gt_60']:.2f}, \"\n",
        "        f\"frac<30 = {baseline_metrics['frac_lt_30']:.2f}\"\n",
        "    )\n",
        "\n",
        "    # --- ES training loop ---\n",
        "    history = []  # (avg_reward, max_reward)\n",
        "    def train_eval_fn(num_sequences, seq_length, temperature):\n",
        "        return evaluate_model_once(\n",
        "            num_sequences=num_sequences,\n",
        "            seq_length=seq_length,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "\n",
        "    for step in range(1, TRAIN_NUM_STEPS + 1):\n",
        "        avg_reward, max_reward, all_rewards = es_step_eggroll(\n",
        "            lm_model,\n",
        "            trained_linear_weights_global,\n",
        "            rank=TRAIN_RANK,\n",
        "            sigma=TRAIN_SIGMA,\n",
        "            pop_size=TRAIN_POP_SIZE,\n",
        "            lr=TRAIN_LR,\n",
        "            device=device,\n",
        "            eval_fn=train_eval_fn,\n",
        "            num_sequences=TRAIN_NUM_SEQUENCES,\n",
        "            seq_length=TRAIN_SEQ_LENGTH,\n",
        "            temperature=TRAIN_TEMPERATURE,\n",
        "        )\n",
        "        history.append((avg_reward, max_reward))\n",
        "        print(\n",
        "            f\"[Exp {exp_id} | Step {step:02d}] \"\n",
        "            f\"avg = {avg_reward:.2f}, max = {max_reward:.2f}\"\n",
        "        )\n",
        "\n",
        "    # --- Evaluate post-training distribution ---\n",
        "    post_metrics = evaluate_model_distribution(\n",
        "        n_sequences=POST_N_SEQS,\n",
        "        seq_length=EVAL_SEQ_LENGTH,\n",
        "        temperature=EVAL_TEMPERATURE,\n",
        "    )\n",
        "    print(\n",
        "        f\"[Exp {exp_id}] Post-train mean pLDDT = {post_metrics['mean']:.2f} \"\n",
        "        f\"+/- {post_metrics['std']:.2f}, \"\n",
        "        f\"frac>60 = {post_metrics['frac_gt_60']:.2f}, \"\n",
        "        f\"frac<30 = {post_metrics['frac_lt_30']:.2f}\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"exp_id\": exp_id,\n",
        "        \"seed\": seed,\n",
        "        \"baseline\": baseline_metrics,\n",
        "        \"post\": post_metrics,\n",
        "        \"history\": history,\n",
        "    }\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Run multiple experiments with different seeds\n",
        "##########################################\n",
        "\n",
        "experiment_results = []\n",
        "\n",
        "base_seed = 12345  # change this if you like\n",
        "for i in range(N_EXPERIMENTS):\n",
        "    seed = base_seed + i\n",
        "    res = run_es_experiment(exp_id=i, seed=seed)\n",
        "    experiment_results.append(res)\n",
        "\n",
        "print(\"\\n=== Finished all experiments ===\")\n"
      ],
      "metadata": {
        "id": "QrSc0RZvcwrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Aggregate baseline vs post across experiments\n",
        "\n",
        "baseline_means = [r[\"baseline\"][\"mean\"] for r in experiment_results]\n",
        "post_means     = [r[\"post\"][\"mean\"] for r in experiment_results]\n",
        "\n",
        "baseline_fracs_gt60 = [r[\"baseline\"][\"frac_gt_60\"] for r in experiment_results]\n",
        "post_fracs_gt60     = [r[\"post\"][\"frac_gt_60\"] for r in experiment_results]\n",
        "\n",
        "baseline_fracs_lt30 = [r[\"baseline\"][\"frac_lt_30\"] for r in experiment_results]\n",
        "post_fracs_lt30     = [r[\"post\"][\"frac_lt_30\"] for r in experiment_results]\n",
        "\n",
        "def summarize(arr):\n",
        "    arr = np.array(arr, dtype=np.float32)\n",
        "    return float(arr.mean()), float(arr.std(ddof=1) if len(arr) > 1 else 0.0)\n",
        "\n",
        "b_mean, b_std = summarize(baseline_means)\n",
        "p_mean, p_std = summarize(post_means)\n",
        "\n",
        "b_gt60_mean, b_gt60_std = summarize(baseline_fracs_gt60)\n",
        "p_gt60_mean, p_gt60_std = summarize(post_fracs_gt60)\n",
        "\n",
        "b_lt30_mean, b_lt30_std = summarize(baseline_fracs_lt30)\n",
        "p_lt30_mean, p_lt30_std = summarize(post_fracs_lt30)\n",
        "\n",
        "print(\"=== Aggregate over experiments ===\")\n",
        "print(f\"Baseline mean pLDDT: {b_mean:.2f} +/- {b_std:.2f}\")\n",
        "print(f\"Post-train mean pLDDT: {p_mean:.2f} +/- {p_std:.2f}\")\n",
        "print()\n",
        "print(f\"Baseline frac>60: {b_gt60_mean:.2f} +/- {b_gt60_std:.2f}\")\n",
        "print(f\"Post-train  frac>60: {p_gt60_mean:.2f} +/- {p_gt60_std:.2f}\")\n",
        "print()\n",
        "print(f\"Baseline frac<30: {b_lt30_mean:.2f} +/- {b_lt30_std:.2f}\")\n",
        "print(f\"Post-train  frac<30: {p_lt30_mean:.2f} +/- {p_lt30_std:.2f}\")\n",
        "\n",
        "##########################################\n",
        "# Optional: visualize distributions for one experiment\n",
        "##########################################\n",
        "\n",
        "exp_to_plot = 0  # index of experiment to inspect\n",
        "\n",
        "baseline_scores = experiment_results[exp_to_plot][\"baseline\"][\"scores\"]\n",
        "post_scores     = experiment_results[exp_to_plot][\"post\"][\"scores\"]\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.hist(baseline_scores, bins=15, alpha=0.6, label=\"baseline\")\n",
        "plt.hist(post_scores, bins=15, alpha=0.6, label=\"post-train\")\n",
        "plt.xlabel(\"pLDDT\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(f\"pLDDT distribution (experiment {exp_to_plot})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "##########################################\n",
        "# Optional: plot ES reward curves for one experiment\n",
        "##########################################\n",
        "\n",
        "history0 = experiment_results[exp_to_plot][\"history\"]\n",
        "avg_hist = [a for (a, m) in history0]\n",
        "max_hist = [m for (a, m) in history0]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(avg_hist, label=\"avg reward\")\n",
        "plt.plot(max_hist, label=\"max reward\")\n",
        "plt.xlabel(\"ES step\")\n",
        "plt.ylabel(\"Reward (mean pLDDT)\")\n",
        "plt.title(f\"ES reward during training (experiment {exp_to_plot})\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v5TJHcwczFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "##########################################\n",
        "# Early-stopping / checkpoint config\n",
        "##########################################\n",
        "\n",
        "ES_VALID_N_SEQS   = 10   # small validation sample size\n",
        "ES_VALID_EVERY    = 2    # run validation every k ES steps\n",
        "ES_PATIENCE       = 4    # stop if no improvement for this many validations\n",
        "ES_MIN_STEPS      = 5    # always run at least this many steps before early stop\n",
        "\n",
        "print(\"=== ES early-stopping CONFIG ===\")\n",
        "print(f\"ES_VALID_N_SEQS = {ES_VALID_N_SEQS}\")\n",
        "print(f\"ES_VALID_EVERY  = {ES_VALID_EVERY}\")\n",
        "print(f\"ES_PATIENCE     = {ES_PATIENCE}\")\n",
        "print(f\"ES_MIN_STEPS    = {ES_MIN_STEPS}\")\n",
        "\n",
        "\n",
        "def run_es_experiment_with_checkpoint(exp_id: int, seed: int):\n",
        "    \"\"\"\n",
        "    ES experiment with:\n",
        "      1) reset to base_state_dict\n",
        "      2) baseline evaluation\n",
        "      3) ES training with:\n",
        "         - best-checkpoint tracking\n",
        "         - optional early stopping\n",
        "      4) post-training evaluation using BEST checkpoint\n",
        "    Requires:\n",
        "      - lm_model, device\n",
        "      - base_state_dict\n",
        "      - evaluate_model_distribution()\n",
        "      - es_step_eggroll()\n",
        "      - trained_linear_weights_global\n",
        "      - TRAIN_* hyperparameters\n",
        "    \"\"\"\n",
        "    print(f\"\\n====================\")\n",
        "    print(f\"Starting ES experiment {exp_id} with seed {seed}\")\n",
        "    print(f\"====================\")\n",
        "\n",
        "    # --- Set seeds for reproducibility ---\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # --- Reset model to base state ---\n",
        "    lm_model.load_state_dict(base_state_dict)\n",
        "    lm_model.to(device)\n",
        "    lm_model.eval()\n",
        "\n",
        "    # --- Evaluate baseline distribution ---\n",
        "    baseline_metrics = evaluate_model_distribution(\n",
        "        n_sequences=BASELINE_N_SEQS,\n",
        "        seq_length=EVAL_SEQ_LENGTH,\n",
        "        temperature=EVAL_TEMPERATURE,\n",
        "    )\n",
        "    print(\n",
        "        f\"[Exp {exp_id}] Baseline mean pLDDT = {baseline_metrics['mean']:.2f} \"\n",
        "        f\"+/- {baseline_metrics['std']:.2f}, \"\n",
        "        f\"frac>60 = {baseline_metrics['frac_gt_60']:.2f}, \"\n",
        "        f\"frac<30 = {baseline_metrics['frac_lt_30']:.2f}\"\n",
        "    )\n",
        "\n",
        "    # --- ES training loop with best checkpoint ---\n",
        "    history = []  # (avg_reward, max_reward)\n",
        "    best_valid_mean = -1e9\n",
        "    best_state_dict = None\n",
        "    best_step = 0\n",
        "    n_valid_calls = 0\n",
        "\n",
        "    def train_eval_fn(num_sequences, seq_length, temperature):\n",
        "        return evaluate_model_once(\n",
        "            num_sequences=num_sequences,\n",
        "            seq_length=seq_length,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "\n",
        "    for step in range(1, TRAIN_NUM_STEPS + 1):\n",
        "        avg_reward, max_reward, all_rewards = es_step_eggroll(\n",
        "            lm_model,\n",
        "            trained_linear_weights_global,\n",
        "            rank=TRAIN_RANK,\n",
        "            sigma=TRAIN_SIGMA,\n",
        "            pop_size=TRAIN_POP_SIZE,\n",
        "            lr=TRAIN_LR,\n",
        "            device=device,\n",
        "            eval_fn=train_eval_fn,\n",
        "            num_sequences=TRAIN_NUM_SEQUENCES,\n",
        "            seq_length=TRAIN_SEQ_LENGTH,\n",
        "            temperature=TRAIN_TEMPERATURE,\n",
        "        )\n",
        "        history.append((avg_reward, max_reward))\n",
        "        print(\n",
        "            f\"[Exp {exp_id} | Step {step:02d}] \"\n",
        "            f\"avg = {avg_reward:.2f}, max = {max_reward:.2f}\"\n",
        "        )\n",
        "\n",
        "        # ---- Validation + checkpointing ----\n",
        "        do_valid = (step % ES_VALID_EVERY == 0) or (step == TRAIN_NUM_STEPS)\n",
        "        if do_valid:\n",
        "            n_valid_calls += 1\n",
        "            valid_metrics = evaluate_model_distribution(\n",
        "                n_sequences=ES_VALID_N_SEQS,\n",
        "                seq_length=EVAL_SEQ_LENGTH,\n",
        "                temperature=EVAL_TEMPERATURE,\n",
        "            )\n",
        "            valid_mean = valid_metrics[\"mean\"]\n",
        "            print(\n",
        "                f\"[Exp {exp_id} | Step {step:02d}] \"\n",
        "                f\"VALID mean pLDDT = {valid_mean:.2f}\"\n",
        "            )\n",
        "\n",
        "            # update best checkpoint\n",
        "            if valid_mean > best_valid_mean:\n",
        "                best_valid_mean = valid_mean\n",
        "                best_state_dict = {\n",
        "                    k: v.detach().cpu().clone() for k, v in lm_model.state_dict().items()\n",
        "                }\n",
        "                best_step = step\n",
        "                print(\n",
        "                    f\"[Exp {exp_id}] New BEST checkpoint at step {step} \"\n",
        "                    f\"(valid mean = {valid_mean:.2f})\"\n",
        "                )\n",
        "\n",
        "            # early stopping: if we have run at least ES_MIN_STEPS and\n",
        "            # no improvement for ES_PATIENCE validations\n",
        "            if step >= ES_MIN_STEPS:\n",
        "                steps_since_best = step - best_step\n",
        "                # number of validations since best\n",
        "                val_since_best = steps_since_best // ES_VALID_EVERY\n",
        "                if val_since_best >= ES_PATIENCE:\n",
        "                    print(\n",
        "                        f\"[Exp {exp_id}] Early stopping at step {step} \"\n",
        "                        f\"(no improvement for {val_since_best} validations).\"\n",
        "                    )\n",
        "                    break\n",
        "\n",
        "    # If we never improved, fall back to final state\n",
        "    if best_state_dict is None:\n",
        "        print(f\"[Exp {exp_id}] No improvement found; using final model.\")\n",
        "    else:\n",
        "        print(\n",
        "            f\"[Exp {exp_id}] Loading BEST checkpoint from step {best_step} \"\n",
        "            f\"(valid mean = {best_valid_mean:.2f})\"\n",
        "        )\n",
        "        lm_model.load_state_dict(best_state_dict)\n",
        "        lm_model.to(device)\n",
        "        lm_model.eval()\n",
        "\n",
        "    # --- Evaluate post-training distribution from BEST checkpoint ---\n",
        "    post_metrics = evaluate_model_distribution(\n",
        "        n_sequences=POST_N_SEQS,\n",
        "        seq_length=EVAL_SEQ_LENGTH,\n",
        "        temperature=EVAL_TEMPERATURE,\n",
        "    )\n",
        "    print(\n",
        "        f\"[Exp {exp_id}] Post-train mean pLDDT = {post_metrics['mean']:.2f} \"\n",
        "        f\"+/- {post_metrics['std']:.2f}, \"\n",
        "        f\"frac>60 = {post_metrics['frac_gt_60']:.2f}, \"\n",
        "        f\"frac<30 = {post_metrics['frac_lt_30']:.2f}\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"exp_id\": exp_id,\n",
        "        \"seed\": seed,\n",
        "        \"baseline\": baseline_metrics,\n",
        "        \"post\": post_metrics,\n",
        "        \"history\": history,\n",
        "        \"best_step\": best_step,\n",
        "        \"best_valid_mean\": best_valid_mean,\n",
        "    }"
      ],
      "metadata": {
        "id": "0toBRy62c1C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "experiment_results_es = []\n",
        "\n",
        "base_seed = 12345\n",
        "for i in range(N_EXPERIMENTS):\n",
        "    seed = base_seed + i\n",
        "    res = run_es_experiment_with_checkpoint(exp_id=i, seed=seed)\n",
        "    experiment_results_es.append(res)\n",
        "\n",
        "print(\"\\n=== Finished ES experiments with checkpointing ===\")"
      ],
      "metadata": {
        "id": "lgv_uyedyFA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "##########################################\n",
        "# Sequence-space evolution baseline config\n",
        "##########################################\n",
        "\n",
        "SEQ_EVO_POP_SIZE      = TRAIN_POP_SIZE * TRAIN_NUM_SEQUENCES  # roughly match ES\n",
        "SEQ_EVO_N_GENERATIONS = TRAIN_NUM_STEPS                       # roughly match ES\n",
        "SEQ_EVO_N_PARENTS     = max(4, SEQ_EVO_POP_SIZE // 4)         # top fraction as parents\n",
        "SEQ_EVO_MUT_RATE      = 0.05   # per-position mutation probability\n",
        "SEQ_EVO_SEQ_LENGTH    = TRAIN_SEQ_LENGTH\n",
        "SEQ_EVO_TEMPERATURE   = TRAIN_TEMPERATURE\n",
        "\n",
        "print(\"=== Sequence-space evolution CONFIG ===\")\n",
        "print(f\"POP_SIZE      = {SEQ_EVO_POP_SIZE}\")\n",
        "print(f\"N_GENERATIONS = {SEQ_EVO_N_GENERATIONS}\")\n",
        "print(f\"N_PARENTS     = {SEQ_EVO_N_PARENTS}\")\n",
        "print(f\"MUT_RATE      = {SEQ_EVO_MUT_RATE}\")\n",
        "print(f\"SEQ_LENGTH    = {SEQ_EVO_SEQ_LENGTH}\")\n",
        "\n",
        "\n",
        "def mutate_sequence(seq: str, mut_rate: float) -> str:\n",
        "    \"\"\"\n",
        "    Simple point-mutation operator on amino acid sequences.\n",
        "    Each position mutates with probability mut_rate to a random AA (uniform).\n",
        "    \"\"\"\n",
        "    global AA_LETTERS\n",
        "    aa_list = list(seq)\n",
        "    for i in range(len(aa_list)):\n",
        "        if random.random() < mut_rate:\n",
        "            aa_list[i] = random.choice(AA_LETTERS)\n",
        "    return \"\".join(aa_list)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sequence_space_evolution_baseline(\n",
        "    pop_size: int,\n",
        "    n_generations: int,\n",
        "    n_parents: int,\n",
        "    seq_length: int,\n",
        "    temperature: float,\n",
        "    mut_rate: float,\n",
        "    seed: int = 0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generic sequence-space evolution baseline:\n",
        "      - Initialize population from the *base* lm_model (base_state_dict)\n",
        "      - For n_generations:\n",
        "          * evaluate with ESMFold\n",
        "          * select top n_parents\n",
        "          * generate new population by mutating parents\n",
        "      - Track history of (avg_score, max_score) and best sequence found.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Running sequence-space evolution baseline (seed={seed}) ===\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Ensure lm_model is in base state (fairness)\n",
        "    lm_model.load_state_dict(base_state_dict)\n",
        "    lm_model.to(device)\n",
        "    lm_model.eval()\n",
        "\n",
        "    # --- Initialize population from base LM ---\n",
        "    population = []\n",
        "    for _ in range(pop_size):\n",
        "        seq = sample_sequence_from_lm(\n",
        "            lm_model,\n",
        "            alphabet,\n",
        "            length=seq_length,\n",
        "            temperature=temperature,\n",
        "            device=device,\n",
        "        )\n",
        "        population.append(seq)\n",
        "\n",
        "    best_seq = None\n",
        "    best_score = -1e9\n",
        "    history = []  # (avg_score, max_score)\n",
        "\n",
        "    for gen in range(n_generations):\n",
        "        scores = []\n",
        "        for seq in population:\n",
        "            sc = evaluate_sequence_with_esmfold(\n",
        "                seq, fold_model=fold_model, device=device\n",
        "            )\n",
        "            scores.append(sc)\n",
        "\n",
        "        scores = np.array(scores, dtype=np.float32)\n",
        "        avg_sc = float(scores.mean())\n",
        "        max_sc = float(scores.max())\n",
        "        history.append((avg_sc, max_sc))\n",
        "\n",
        "        print(\n",
        "            f\"[SeqEvo | Gen {gen:02d}] \"\n",
        "            f\"avg = {avg_sc:.2f}, max = {max_sc:.2f}\"\n",
        "        )\n",
        "\n",
        "        # Update global best\n",
        "        best_idx = int(scores.argmax())\n",
        "        if max_sc > best_score:\n",
        "            best_score = max_sc\n",
        "            best_seq = population[best_idx]\n",
        "            print(f\"[SeqEvo] New best sequence with pLDDT = {best_score:.2f}\")\n",
        "\n",
        "        # Last generation: no need to create next population\n",
        "        if gen == n_generations - 1:\n",
        "            break\n",
        "\n",
        "        # --- Select parents & generate next-generation population ---\n",
        "        parent_indices = scores.argsort()[::-1][:n_parents]\n",
        "        parents = [population[i] for i in parent_indices]\n",
        "\n",
        "        new_population = []\n",
        "\n",
        "        # Elitism: carry over the best parent unchanged\n",
        "        new_population.append(parents[0])\n",
        "\n",
        "        # Fill the rest with mutated children\n",
        "        while len(new_population) < pop_size:\n",
        "            parent = random.choice(parents)\n",
        "            child = mutate_sequence(parent, mut_rate)\n",
        "            new_population.append(child)\n",
        "\n",
        "        population = new_population\n",
        "\n",
        "    return {\n",
        "        \"history\": history,\n",
        "        \"best_seq\": best_seq,\n",
        "        \"best_score\": best_score,\n",
        "        \"final_population\": population,\n",
        "    }"
      ],
      "metadata": {
        "id": "S5oOLa9AyIAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "seq_evo_result = sequence_space_evolution_baseline(\n",
        "    pop_size=SEQ_EVO_POP_SIZE,\n",
        "    n_generations=SEQ_EVO_N_GENERATIONS,\n",
        "    n_parents=SEQ_EVO_N_PARENTS,\n",
        "    seq_length=SEQ_EVO_SEQ_LENGTH,\n",
        "    temperature=SEQ_EVO_TEMPERATURE,\n",
        "    mut_rate=SEQ_EVO_MUT_RATE,\n",
        "    seed=999,\n",
        ")\n",
        "\n",
        "print(\"\\n=== Sequence-space evolution summary ===\")\n",
        "print(f\"Best pLDDT found: {seq_evo_result['best_score']:.2f}\")\n",
        "print(f\"Best sequence:\\n{seq_evo_result['best_seq']}\")"
      ],
      "metadata": {
        "id": "_QBzMvqxyLR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example: compare ES experiment 0 vs sequence-space baseline\n",
        "es_hist = experiment_results_es[0][\"history\"]  # (avg, max) per ES step\n",
        "es_avg = [a for (a, m) in es_hist]\n",
        "es_max = [m for (a, m) in es_hist]\n",
        "\n",
        "seq_hist = seq_evo_result[\"history\"]\n",
        "seq_avg = [a for (a, m) in seq_hist]\n",
        "seq_max = [m for (a, m) in seq_hist]\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(es_avg, label=\"ES avg\")\n",
        "plt.plot(es_max, label=\"ES max\")\n",
        "plt.plot(seq_avg, label=\"SeqEvo avg\", linestyle=\"--\")\n",
        "plt.plot(seq_max, label=\"SeqEvo max\", linestyle=\"--\")\n",
        "plt.xlabel(\"Step / Generation\")\n",
        "plt.ylabel(\"pLDDT\")\n",
        "plt.title(\"ES vs sequence-space evolution (single seed)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bvmL7-fOyMv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Post-ES utilities:\n",
        "- Multi-objective reward (pLDDT + LM prior)\n",
        "- ESMFold compute accounting for ES vs SeqEvo\n",
        "- Multi-seed SeqEvo baseline\n",
        "- Diversity analysis (sequence identity & entropy)\n",
        "- Example small ES vs SeqEvo comparison using the new objective\n",
        "\n",
        "This cell assumes you already defined:\n",
        "  - lm_model, alphabet, AA_LETTERS, device\n",
        "  - sample_sequence_from_lm(...)\n",
        "  - evaluate_sequence_with_esmfold(...)\n",
        "  - es_step_eggroll(...)\n",
        "  - evaluate_model_distribution(...)\n",
        "  - trained_linear_weights_global\n",
        "  - base_state_dict\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "##########################################\n",
        "# 1. Global counters for ESMFold calls\n",
        "##########################################\n",
        "\n",
        "ESMFOLD_CALLS_ES      = 0  # calls used by ES experiments\n",
        "ESMFOLD_CALLS_SEQEVO  = 0  # calls used by SeqEvo experiments\n",
        "\n",
        "def reset_esmfold_counters():\n",
        "    global ESMFOLD_CALLS_ES, ESMFOLD_CALLS_SEQEVO\n",
        "    ESMFOLD_CALLS_ES = 0\n",
        "    ESMFOLD_CALLS_SEQEVO = 0\n",
        "\n",
        "print(\"Initialized ESMFold call counters.\")\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 2. LM pseudo-likelihood (prior term)\n",
        "##########################################\n",
        "\n",
        "@torch.no_grad()\n",
        "def lm_pseudo_log_prob_per_residue(seq: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute a (rough) pseudo log-likelihood per residue for a sequence under ESM-2.\n",
        "\n",
        "    ESM-2 is a masked LM, so we approximate:\n",
        "      log p(x) ≈ sum_i log p(x_i | x_{-i})\n",
        "    by masking one position at a time and reading off the probability of the\n",
        "    true amino acid.\n",
        "\n",
        "    Returns natural-log average per residue (higher is better).\n",
        "    \"\"\"\n",
        "    tokens = torch.full(\n",
        "        (1, len(seq) + 2),\n",
        "        fill_value=alphabet.mask_idx,\n",
        "        device=device,\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "    tokens[0, 0] = alphabet.cls_idx\n",
        "    tokens[0, -1] = alphabet.eos_idx\n",
        "    # fill true sequence\n",
        "    for i, aa in enumerate(seq, start=1):\n",
        "        tokens[0, i] = alphabet.get_idx(aa)\n",
        "\n",
        "    log_probs = []\n",
        "    for pos in range(1, len(seq) + 1):\n",
        "        orig = tokens[0, pos].item()\n",
        "        tokens[0, pos] = alphabet.mask_idx\n",
        "        out = lm_model(tokens, repr_layers=[], return_contacts=False)\n",
        "        logits = out[\"logits\"][0, pos]  # vocab\n",
        "        logp = torch.log_softmax(logits, dim=-1)[orig]\n",
        "        log_probs.append(float(logp))\n",
        "        tokens[0, pos] = orig\n",
        "\n",
        "    return float(np.mean(log_probs))\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 3. Multi-objective reward\n",
        "##########################################\n",
        "\n",
        "def compute_multiobjective_reward(\n",
        "    seq: str,\n",
        "    weight_lm_prior: float = 0.1,\n",
        "    caller: str = \"ES\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Composite reward:\n",
        "        R(seq) = pLDDT(seq)  + weight_lm_prior * normalized_LM_prior(seq)\n",
        "\n",
        "    where normalized_LM_prior is log p(x) / log(1/20) so that a uniform model ~0\n",
        "    and a better-than-random model >0.\n",
        "\n",
        "    caller: \"ES\" or \"SEQEVO\" -> used to increment correct ESMFold counter.\n",
        "    \"\"\"\n",
        "    global ESMFOLD_CALLS_ES, ESMFOLD_CALLS_SEQEVO\n",
        "\n",
        "    # 1) structural term: mean pLDDT from ESMFold\n",
        "    plddt = evaluate_sequence_with_esmfold(seq, fold_model=fold_model, device=device)\n",
        "\n",
        "    if caller.upper() == \"ES\":\n",
        "        ESMFOLD_CALLS_ES += 1\n",
        "    elif caller.upper() == \"SEQEVO\":\n",
        "        ESMFOLD_CALLS_SEQEVO += 1\n",
        "\n",
        "    # 2) LM prior term\n",
        "    lm_logp = lm_pseudo_log_prob_per_residue(seq)\n",
        "    # normalize vs uniform AA distribution: log(1/20) = -log(20)\n",
        "    norm_lm = lm_logp / (-math.log(1.0 / 20.0) + 1e-8)\n",
        "\n",
        "    reward = float(plddt + weight_lm_prior * norm_lm)\n",
        "\n",
        "    return reward, plddt, lm_logp, norm_lm\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 4. ES training with the new objective & compute tracking\n",
        "##########################################\n",
        "\n",
        "def evaluate_model_once_objective(\n",
        "    num_sequences: int,\n",
        "    seq_length: int,\n",
        "    temperature: float,\n",
        "    weight_lm_prior: float = 0.1,\n",
        "):\n",
        "    \"\"\"\n",
        "    For current parameter setting of lm_model, generate num_sequences sequences,\n",
        "    evaluate composite reward for each, and return (avg_reward, list_rewards).\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for _ in range(num_sequences):\n",
        "        seq = sample_sequence_from_lm(\n",
        "            lm_model,\n",
        "            alphabet,\n",
        "            length=seq_length,\n",
        "            temperature=temperature,\n",
        "            device=device,\n",
        "        )\n",
        "        r, plddt, lm_logp, norm_lm = compute_multiobjective_reward(\n",
        "            seq,\n",
        "            weight_lm_prior=weight_lm_prior,\n",
        "            caller=\"ES\",\n",
        "        )\n",
        "        rewards.append(r)\n",
        "    return float(np.mean(rewards)), rewards\n",
        "\n",
        "\n",
        "def run_es_multiobjective_experiment(\n",
        "    exp_id: int,\n",
        "    seed: int,\n",
        "    train_steps: int,\n",
        "    train_pop_size: int,\n",
        "    train_num_sequences: int,\n",
        "    train_seq_length: int,\n",
        "    train_temperature: float,\n",
        "    train_sigma: float,\n",
        "    train_lr: float,\n",
        "    weight_lm_prior: float = 0.1,\n",
        "):\n",
        "    \"\"\"\n",
        "    Single ES experiment with the new composite objective.\n",
        "    Uses the existing es_step_eggroll but plugs in evaluate_model_once_objective.\n",
        "    \"\"\"\n",
        "    print(f\"\\n====================\")\n",
        "    print(f\"Starting ES multi-objective experiment {exp_id} (seed={seed})\")\n",
        "    print(f\"====================\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # reset model to base state\n",
        "    lm_model.load_state_dict(base_state_dict)\n",
        "    lm_model.to(device)\n",
        "    lm_model.eval()\n",
        "\n",
        "    history = []\n",
        "\n",
        "    def train_eval_fn(num_sequences, seq_length, temperature):\n",
        "        return evaluate_model_once_objective(\n",
        "            num_sequences=num_sequences,\n",
        "            seq_length=seq_length,\n",
        "            temperature=temperature,\n",
        "            weight_lm_prior=weight_lm_prior,\n",
        "        )\n",
        "\n",
        "    for step in range(1, train_steps + 1):\n",
        "        avg_reward, max_reward, all_rewards = es_step_eggroll(\n",
        "            lm_model,\n",
        "            trained_linear_weights_global,\n",
        "            rank=TRAIN_RANK,\n",
        "            sigma=train_sigma,\n",
        "            pop_size=train_pop_size,\n",
        "            lr=train_lr,\n",
        "            device=device,\n",
        "            eval_fn=train_eval_fn,\n",
        "            num_sequences=train_num_sequences,\n",
        "            seq_length=train_seq_length,\n",
        "            temperature=train_temperature,\n",
        "        )\n",
        "        history.append((avg_reward, max_reward))\n",
        "        print(\n",
        "            f\"[ES-Obj Exp {exp_id} | Step {step:02d}] \"\n",
        "            f\"avg_reward={avg_reward:.2f}, max_reward={max_reward:.2f}\"\n",
        "        )\n",
        "\n",
        "    # after training, sample some sequences and measure pLDDT & LM prior separately\n",
        "    eval_metrics = evaluate_model_distribution(\n",
        "        n_sequences=30,\n",
        "        seq_length=train_seq_length,\n",
        "        temperature=train_temperature,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"exp_id\": exp_id,\n",
        "        \"seed\": seed,\n",
        "        \"history\": history,\n",
        "        \"eval_metrics\": eval_metrics,\n",
        "    }\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 5. SeqEvo baseline with the same objective & compute tracking\n",
        "##########################################\n",
        "\n",
        "def mutate_sequence_with_mask(seq: str, mut_rate: float, mutable_mask=None) -> str:\n",
        "    \"\"\"\n",
        "    Point mutation with optional mask:\n",
        "      - If mutable_mask is None: any position can mutate.\n",
        "      - Else: only positions with mutable_mask[i] == True can mutate.\n",
        "    \"\"\"\n",
        "    global AA_LETTERS\n",
        "    aa_list = list(seq)\n",
        "    L = len(aa_list)\n",
        "    if mutable_mask is None:\n",
        "        mutable_mask = [True] * L\n",
        "\n",
        "    for i in range(L):\n",
        "        if not mutable_mask[i]:\n",
        "            continue\n",
        "        if random.random() < mut_rate:\n",
        "            aa_list[i] = random.choice(AA_LETTERS)\n",
        "    return \"\".join(aa_list)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sequence_space_evolution_objective(\n",
        "    exp_id: int,\n",
        "    seed: int,\n",
        "    pop_size: int,\n",
        "    n_generations: int,\n",
        "    n_parents: int,\n",
        "    seq_length: int,\n",
        "    temperature: float,\n",
        "    mut_rate: float,\n",
        "    weight_lm_prior: float = 0.1,\n",
        "    mutable_mask=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    SeqEvo baseline using the same composite reward as ES.\n",
        "\n",
        "    mutable_mask: optional list[bool] of length seq_length;\n",
        "                  False positions are frozen (scaffold), only others mutate.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== SeqEvo multi-objective experiment {exp_id} (seed={seed}) ===\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # ensure base LM (for fairness)\n",
        "    lm_model.load_state_dict(base_state_dict)\n",
        "    lm_model.to(device)\n",
        "    lm_model.eval()\n",
        "\n",
        "    # Initialize population\n",
        "    population = []\n",
        "    for _ in range(pop_size):\n",
        "        seq = sample_sequence_from_lm(\n",
        "            lm_model,\n",
        "            alphabet,\n",
        "            length=seq_length,\n",
        "            temperature=temperature,\n",
        "            device=device,\n",
        "        )\n",
        "        population.append(seq)\n",
        "\n",
        "    best_seq = None\n",
        "    best_reward = -1e9\n",
        "    best_plddt = -1e9\n",
        "    history = []\n",
        "\n",
        "    for gen in range(n_generations):\n",
        "        rewards = []\n",
        "        plddts = []\n",
        "        for seq in population:\n",
        "            r, plddt, lm_logp, norm_lm = compute_multiobjective_reward(\n",
        "                seq,\n",
        "                weight_lm_prior=weight_lm_prior,\n",
        "                caller=\"SEQEVO\",\n",
        "            )\n",
        "            rewards.append(r)\n",
        "            plddts.append(plddt)\n",
        "\n",
        "        rewards = np.array(rewards, dtype=np.float32)\n",
        "        plddts = np.array(plddts, dtype=np.float32)\n",
        "\n",
        "        avg_r = float(rewards.mean())\n",
        "        max_r = float(rewards.max())\n",
        "        history.append((avg_r, max_r))\n",
        "\n",
        "        print(\n",
        "            f\"[SeqEvo-Obj Exp {exp_id} | Gen {gen:02d}] \"\n",
        "            f\"avg_reward={avg_r:.2f}, max_reward={max_r:.2f}, \"\n",
        "            f\"avg_plddt={float(plddts.mean()):.2f}\"\n",
        "        )\n",
        "\n",
        "        # update best\n",
        "        idx_best = int(rewards.argmax())\n",
        "        if rewards[idx_best] > best_reward:\n",
        "            best_reward = float(rewards[idx_best])\n",
        "            best_plddt = float(plddts[idx_best])\n",
        "            best_seq = population[idx_best]\n",
        "            print(\n",
        "                f\"[SeqEvo-Obj Exp {exp_id}] New best: \"\n",
        "                f\"reward={best_reward:.2f}, pLDDT={best_plddt:.2f}\"\n",
        "            )\n",
        "\n",
        "        if gen == n_generations - 1:\n",
        "            break\n",
        "\n",
        "        # parent selection\n",
        "        parent_indices = rewards.argsort()[::-1][:n_parents]\n",
        "        parents = [population[i] for i in parent_indices]\n",
        "\n",
        "        new_population = []\n",
        "        # elitism\n",
        "        new_population.append(parents[0])\n",
        "\n",
        "        while len(new_population) < pop_size:\n",
        "            parent = random.choice(parents)\n",
        "            child = mutate_sequence_with_mask(parent, mut_rate, mutable_mask)\n",
        "            new_population.append(child)\n",
        "\n",
        "        population = new_population\n",
        "\n",
        "    return {\n",
        "        \"exp_id\": exp_id,\n",
        "        \"seed\": seed,\n",
        "        \"history\": history,\n",
        "        \"best_seq\": best_seq,\n",
        "        \"best_reward\": best_reward,\n",
        "        \"best_plddt\": best_plddt,\n",
        "        \"final_population\": population,\n",
        "    }\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 6. Diversity analysis\n",
        "##########################################\n",
        "\n",
        "def sequence_identity(s1: str, s2: str) -> float:\n",
        "    \"\"\"Fraction of identical positions between two equal-length sequences.\"\"\"\n",
        "    assert len(s1) == len(s2)\n",
        "    same = sum(a == b for a, b in zip(s1, s2))\n",
        "    return same / len(s1)\n",
        "\n",
        "\n",
        "def analyze_sequence_set(seqs, label=\"set\"):\n",
        "    \"\"\"\n",
        "    Compute simple diversity metrics:\n",
        "      - mean pairwise identity\n",
        "      - per-position entropy\n",
        "    \"\"\"\n",
        "    if len(seqs) == 0:\n",
        "        print(f\"[Diversity] {label}: no sequences.\")\n",
        "        return {}\n",
        "\n",
        "    L = len(seqs[0])\n",
        "    seqs = [s for s in seqs if len(s) == L]\n",
        "    n = len(seqs)\n",
        "\n",
        "    # pairwise identities\n",
        "    if n > 1:\n",
        "        ids = []\n",
        "        for i in range(n):\n",
        "            for j in range(i + 1, n):\n",
        "                ids.append(sequence_identity(seqs[i], seqs[j]))\n",
        "        mean_id = float(np.mean(ids))\n",
        "    else:\n",
        "        mean_id = float(\"nan\")\n",
        "\n",
        "    # per-position entropy\n",
        "    entropies = []\n",
        "    for pos in range(L):\n",
        "        column = [s[pos] for s in seqs]\n",
        "        counts = Counter(column)\n",
        "        total = sum(counts.values())\n",
        "        probs = np.array([c / total for c in counts.values()], dtype=np.float32)\n",
        "        ent = -float((probs * np.log2(probs + 1e-12)).sum())\n",
        "        entropies.append(ent)\n",
        "\n",
        "    mean_entropy = float(np.mean(entropies))\n",
        "\n",
        "    print(\n",
        "        f\"[Diversity] {label}: n={n}, \"\n",
        "        f\"mean pairwise identity={mean_id:.3f}, \"\n",
        "        f\"mean per-pos entropy={mean_entropy:.3f}\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"n\": n,\n",
        "        \"mean_identity\": mean_id,\n",
        "        \"mean_entropy\": mean_entropy,\n",
        "        \"entropies\": entropies,\n",
        "    }\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 7. Example small-scale ES vs SeqEvo objective run\n",
        "##########################################\n",
        "\n",
        "# You can scale these up later; these are modest so it doesn't explode compute.\n",
        "OBJ_ES_N_EXPERIMENTS      = 1\n",
        "OBJ_ES_TRAIN_STEPS        = 5\n",
        "OBJ_ES_POP_SIZE           = 4\n",
        "OBJ_ES_NUM_SEQUENCES      = 2\n",
        "OBJ_ES_SEQ_LENGTH         = 60\n",
        "OBJ_ES_TEMPERATURE        = 1.0\n",
        "OBJ_ES_SIGMA              = TRAIN_SIGMA    # reuse your ES sigma\n",
        "OBJ_ES_LR                 = TRAIN_LR       # reuse your ES lr\n",
        "OBJ_WEIGHT_LM_PRIOR       = 0.1\n",
        "\n",
        "OBJ_SEQEVO_N_EXPERIMENTS  = 1\n",
        "OBJ_SEQEVO_POP_SIZE       = OBJ_ES_POP_SIZE * OBJ_ES_NUM_SEQUENCES\n",
        "OBJ_SEQEVO_GENERATIONS    = OBJ_ES_TRAIN_STEPS\n",
        "OBJ_SEQEVO_N_PARENTS      = max(4, OBJ_SEQEVO_POP_SIZE // 4)\n",
        "OBJ_SEQEVO_MUT_RATE       = 0.05\n",
        "OBJ_SEQEVO_SEQ_LENGTH     = OBJ_ES_SEQ_LENGTH\n",
        "OBJ_SEQEVO_TEMPERATURE    = OBJ_ES_TEMPERATURE\n",
        "\n",
        "print(\"\\n=== Multi-objective small run CONFIG ===\")\n",
        "print(f\"ES: {OBJ_ES_N_EXPERIMENTS} exp, steps={OBJ_ES_TRAIN_STEPS}, pop={OBJ_ES_POP_SIZE}\")\n",
        "print(f\"SeqEvo: {OBJ_SEQEVO_N_EXPERIMENTS} exp, gens={OBJ_SEQEVO_GENERATIONS}, pop={OBJ_SEQEVO_POP_SIZE}\")\n",
        "\n",
        "reset_esmfold_counters()\n",
        "\n",
        "# ---- Run ES multi-objective (small) ----\n",
        "es_obj_results = []\n",
        "base_seed = 2025\n",
        "for i in range(OBJ_ES_N_EXPERIMENTS):\n",
        "    res = run_es_multiobjective_experiment(\n",
        "        exp_id=i,\n",
        "        seed=base_seed + i,\n",
        "        train_steps=OBJ_ES_TRAIN_STEPS,\n",
        "        train_pop_size=OBJ_ES_POP_SIZE,\n",
        "        train_num_sequences=OBJ_ES_NUM_SEQUENCES,\n",
        "        train_seq_length=OBJ_ES_SEQ_LENGTH,\n",
        "        train_temperature=OBJ_ES_TEMPERATURE,\n",
        "        train_sigma=OBJ_ES_SIGMA,\n",
        "        train_lr=OBJ_ES_LR,\n",
        "        weight_lm_prior=OBJ_WEIGHT_LM_PRIOR,\n",
        "    )\n",
        "    es_obj_results.append(res)\n",
        "\n",
        "# sample from final ES model of last experiment for diversity\n",
        "es_sample_seqs = []\n",
        "for _ in range(32):\n",
        "    s = sample_sequence_from_lm(\n",
        "        lm_model,\n",
        "        alphabet,\n",
        "        length=OBJ_ES_SEQ_LENGTH,\n",
        "        temperature=OBJ_ES_TEMPERATURE,\n",
        "        device=device,\n",
        "    )\n",
        "    es_sample_seqs.append(s)\n",
        "\n",
        "# ---- Run SeqEvo multi-objective (small) ----\n",
        "seqevo_obj_results = []\n",
        "for i in range(OBJ_SEQEVO_N_EXPERIMENTS):\n",
        "    res = sequence_space_evolution_objective(\n",
        "        exp_id=i,\n",
        "        seed=base_seed + 100 + i,\n",
        "        pop_size=OBJ_SEQEVO_POP_SIZE,\n",
        "        n_generations=OBJ_SEQEVO_GENERATIONS,\n",
        "        n_parents=OBJ_SEQEVO_N_PARENTS,\n",
        "        seq_length=OBJ_SEQEVO_SEQ_LENGTH,\n",
        "        temperature=OBJ_SEQEVO_TEMPERATURE,\n",
        "        mut_rate=OBJ_SEQEVO_MUT_RATE,\n",
        "        weight_lm_prior=OBJ_WEIGHT_LM_PRIOR,\n",
        "        mutable_mask=None,  # could freeze scaffold here\n",
        "    )\n",
        "    seqevo_obj_results.append(res)\n",
        "\n",
        "seqevo_final_pop = seqevo_obj_results[0][\"final_population\"]\n",
        "\n",
        "print(\"\\n=== ESMFold compute summary ===\")\n",
        "print(f\"ES ESMFold calls:      {ESMFOLD_CALLS_ES}\")\n",
        "print(f\"SeqEvo ESMFold calls:  {ESMFOLD_CALLS_SEQEVO}\")\n",
        "\n",
        "# ---- Diversity comparison ----\n",
        "div_es = analyze_sequence_set(es_sample_seqs, label=\"ES-final-samples\")\n",
        "div_seqevo = analyze_sequence_set(seqevo_final_pop, label=\"SeqEvo-final-population\")\n",
        "\n",
        "# ---- Simple plot of reward curves (single seed) ----\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "es_hist = es_obj_results[0][\"history\"]\n",
        "es_avg = [a for (a, m) in es_hist]\n",
        "es_max = [m for (a, m) in es_hist]\n",
        "\n",
        "seq_hist = seqevo_obj_results[0][\"history\"]\n",
        "seq_avg = [a for (a, m) in seq_hist]\n",
        "seq_max = [m for (a, m) in seq_hist]\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(es_avg, label=\"ES avg\")\n",
        "plt.plot(es_max, label=\"ES max\")\n",
        "plt.plot(seq_avg, label=\"SeqEvo avg\", linestyle=\"--\")\n",
        "plt.plot(seq_max, label=\"SeqEvo max\", linestyle=\"--\")\n",
        "plt.xlabel(\"Step / Generation\")\n",
        "plt.ylabel(\"Multi-objective reward\")\n",
        "plt.title(\"ES vs sequence-space evolution (multi-objective, small run)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BS8Tja5gyOoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Constrained, multi-objective ES vs SeqEvo with multiple seeds.\n",
        "\n",
        "Adds:\n",
        "  - richer composite reward (pLDDT + LM prior - hydrophobic penalty)\n",
        "  - constrained loop regime (fixed scaffold, mutable internal region)\n",
        "  - multi-seed ES vs SeqEvo comparison with matched ESMFold training budget\n",
        "  - summary of rewards and diversity across seeds\n",
        "\n",
        "ASSUMES the previous cell has already defined:\n",
        "  - lm_model, alphabet, AA_LETTERS, device\n",
        "  - sample_sequence_from_lm(...)\n",
        "  - evaluate_sequence_with_esmfold(...)\n",
        "  - ESMFold call counters: ESMFOLD_CALLS_ES, ESMFOLD_CALLS_SEQEVO, reset_esmfold_counters()\n",
        "  - diversity helpers: analyze_sequence_set(...)\n",
        "  - ES machinery: es_step_eggroll(...), trained_linear_weights_global, TRAIN_RANK, TRAIN_SIGMA, TRAIN_LR\n",
        "  - base_state_dict (saved initial LM weights)\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "##########################################\n",
        "# 1. Updated multi-objective reward\n",
        "##########################################\n",
        "\n",
        "HYDROPHOBIC_AA = set(\"AILMFWVY\")\n",
        "\n",
        "\n",
        "def hydrophobic_fraction(seq: str) -> float:\n",
        "    return sum(aa in HYDROPHOBIC_AA for aa in seq) / max(len(seq), 1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def lm_pseudo_log_prob_per_residue(seq: str) -> float:\n",
        "    \"\"\"\n",
        "    Pseudo log-likelihood per residue for a sequence under ESM-2.\n",
        "\n",
        "    Same as in the previous cell, but repeated here to keep this block self-contained.\n",
        "    \"\"\"\n",
        "    tokens = torch.full(\n",
        "        (1, len(seq) + 2),\n",
        "        fill_value=alphabet.mask_idx,\n",
        "        device=device,\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "    tokens[0, 0] = alphabet.cls_idx\n",
        "    tokens[0, -1] = alphabet.eos_idx\n",
        "    for i, aa in enumerate(seq, start=1):\n",
        "        tokens[0, i] = alphabet.get_idx(aa)\n",
        "\n",
        "    log_probs = []\n",
        "    for pos in range(1, len(seq) + 1):\n",
        "        orig = tokens[0, pos].item()\n",
        "        tokens[0, pos] = alphabet.mask_idx\n",
        "        out = lm_model(tokens, repr_layers=[], return_contacts=False)\n",
        "        logits = out[\"logits\"][0, pos]\n",
        "        logp = torch.log_softmax(logits, dim=-1)[orig]\n",
        "        log_probs.append(float(logp))\n",
        "        tokens[0, pos] = orig\n",
        "\n",
        "    return float(np.mean(log_probs))\n",
        "\n",
        "\n",
        "def compute_multiobjective_reward(\n",
        "    seq: str,\n",
        "    weight_lm_prior: float = 0.1,\n",
        "    target_hydro: float = 0.50,\n",
        "    lambda_hydro: float = 20.0,\n",
        "    caller: str = \"ES\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Composite reward used for *both* ES and SeqEvo:\n",
        "\n",
        "      R(seq) =\n",
        "          pLDDT(seq)\n",
        "        + weight_lm_prior * normalized_LM_prior(seq)\n",
        "        - lambda_hydro * |hydrophobic_fraction(seq) - target_hydro|\n",
        "\n",
        "    where normalized_LM_prior ≈ log p(x) / log(1/20), so that\n",
        "    a uniform random model ≈ 0, better models > 0.\n",
        "\n",
        "    We also increment global ESMFold call counters for compute accounting.\n",
        "    \"\"\"\n",
        "    global ESMFOLD_CALLS_ES, ESMFOLD_CALLS_SEQEVO\n",
        "\n",
        "    # 1) ESMFold term\n",
        "    plddt = evaluate_sequence_with_esmfold(seq, fold_model=fold_model, device=device)\n",
        "    if caller.upper() == \"ES\":\n",
        "        ESMFOLD_CALLS_ES += 1\n",
        "    elif caller.upper() == \"SEQEVO\":\n",
        "        ESMFOLD_CALLS_SEQEVO += 1\n",
        "\n",
        "    # 2) LM prior\n",
        "    lm_logp = lm_pseudo_log_prob_per_residue(seq)\n",
        "    norm_lm = lm_logp / (-math.log(1.0 / 20.0) + 1e-8)\n",
        "\n",
        "    # 3) hydrophobicity penalty\n",
        "    h = hydrophobic_fraction(seq)\n",
        "    hydro_pen = lambda_hydro * abs(h - target_hydro)\n",
        "\n",
        "    reward = float(plddt + weight_lm_prior * norm_lm - hydro_pen)\n",
        "    return reward, plddt, lm_logp, norm_lm, h\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 2. Constrained sequence generator (fixed scaffold, mutable loop)\n",
        "##########################################\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_sequence_from_lm_constrained(\n",
        "    model,\n",
        "    alphabet,\n",
        "    base_seq: str,\n",
        "    mutable_mask,\n",
        "    temperature: float = 1.0,\n",
        "    device=device,\n",
        "):\n",
        "    \"\"\"\n",
        "    Sample a sequence of same length as base_seq, keeping positions with\n",
        "    mutable_mask[i] == False fixed to base_seq[i], and resampling the others.\n",
        "\n",
        "    Uses the same \"mask a single position, run LM\" trick as sample_sequence_from_lm.\n",
        "    \"\"\"\n",
        "    L = len(base_seq)\n",
        "    assert len(mutable_mask) == L\n",
        "\n",
        "    tokens = torch.full(\n",
        "        (1, L + 2),\n",
        "        fill_value=alphabet.mask_idx,\n",
        "        device=device,\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "    tokens[0, 0] = alphabet.cls_idx\n",
        "    tokens[0, -1] = alphabet.eos_idx\n",
        "\n",
        "    # initialize tokens to base_seq\n",
        "    for i, aa in enumerate(base_seq, start=1):\n",
        "        tokens[0, i] = alphabet.get_idx(aa)\n",
        "\n",
        "    for pos in range(1, L + 1):\n",
        "        if not mutable_mask[pos - 1]:\n",
        "            # keep scaffold amino acid\n",
        "            continue\n",
        "\n",
        "        tokens[0, pos] = alphabet.mask_idx\n",
        "        out = model(tokens, repr_layers=[], return_contacts=False)\n",
        "        logits = out[\"logits\"][0, pos]\n",
        "\n",
        "        aa_logits = logits[AA_INDICES]\n",
        "        probs = torch.softmax(aa_logits / temperature, dim=-1)\n",
        "        aa_idx = torch.multinomial(probs, num_samples=1)\n",
        "        tok_id = AA_INDICES[aa_idx]\n",
        "        tokens[0, pos] = tok_id\n",
        "\n",
        "    seq_tokens = tokens[0, 1:-1].tolist()\n",
        "    seq = \"\".join(alphabet.get_tok(t) for t in seq_tokens)\n",
        "    return seq\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 3. ES multi-objective experiment (with constraints)\n",
        "##########################################\n",
        "\n",
        "def evaluate_model_once_objective(\n",
        "    num_sequences: int,\n",
        "    seq_length: int,\n",
        "    temperature: float,\n",
        "    weight_lm_prior: float,\n",
        "    target_hydro: float,\n",
        "    lambda_hydro: float,\n",
        "    base_seq: str = None,\n",
        "    mutable_mask=None,\n",
        "):\n",
        "    rewards = []\n",
        "    for _ in range(num_sequences):\n",
        "        if base_seq is None or mutable_mask is None:\n",
        "            seq = sample_sequence_from_lm(\n",
        "                lm_model,\n",
        "                alphabet,\n",
        "                length=seq_length,\n",
        "                temperature=temperature,\n",
        "                device=device,\n",
        "            )\n",
        "        else:\n",
        "            seq = sample_sequence_from_lm_constrained(\n",
        "                lm_model,\n",
        "                alphabet,\n",
        "                base_seq=base_seq,\n",
        "                mutable_mask=mutable_mask,\n",
        "                temperature=temperature,\n",
        "                device=device,\n",
        "            )\n",
        "\n",
        "        r, plddt, lm_logp, norm_lm, h = compute_multiobjective_reward(\n",
        "            seq,\n",
        "            weight_lm_prior=weight_lm_prior,\n",
        "            target_hydro=target_hydro,\n",
        "            lambda_hydro=lambda_hydro,\n",
        "            caller=\"ES\",\n",
        "        )\n",
        "        rewards.append(r)\n",
        "\n",
        "    return float(np.mean(rewards)), rewards\n",
        "\n",
        "\n",
        "def run_es_multiobjective_experiment_constrained(\n",
        "    exp_id: int,\n",
        "    seed: int,\n",
        "    train_steps: int,\n",
        "    train_pop_size: int,\n",
        "    train_num_sequences: int,\n",
        "    seq_length: int,\n",
        "    temperature: float,\n",
        "    sigma: float,\n",
        "    lr: float,\n",
        "    weight_lm_prior: float,\n",
        "    target_hydro: float,\n",
        "    lambda_hydro: float,\n",
        "    base_seq: str,\n",
        "    mutable_mask,\n",
        "):\n",
        "    \"\"\"\n",
        "    Same as run_es_multiobjective_experiment, but:\n",
        "      - uses constrained sampling around base_seq with mutable_mask\n",
        "      - returns best training reward + history\n",
        "    \"\"\"\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"ES constrained exp {exp_id} (seed={seed})\")\n",
        "    print(f\"==============================\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    lm_model.load_state_dict(base_state_dict)\n",
        "    lm_model.to(device)\n",
        "    lm_model.eval()\n",
        "\n",
        "    history = []\n",
        "    best_training_reward = -1e9\n",
        "\n",
        "    def train_eval_fn(num_sequences, seq_length, temperature):\n",
        "        avg_r, rewards = evaluate_model_once_objective(\n",
        "            num_sequences=num_sequences,\n",
        "            seq_length=seq_length,\n",
        "            temperature=temperature,\n",
        "            weight_lm_prior=weight_lm_prior,\n",
        "            target_hydro=target_hydro,\n",
        "            lambda_hydro=lambda_hydro,\n",
        "            base_seq=base_seq,\n",
        "            mutable_mask=mutable_mask,\n",
        "        )\n",
        "        return avg_r, rewards\n",
        "\n",
        "    for step in range(1, train_steps + 1):\n",
        "        avg_reward, max_reward, all_rewards = es_step_eggroll(\n",
        "            lm_model,\n",
        "            trained_linear_weights_global,\n",
        "            rank=TRAIN_RANK,\n",
        "            sigma=sigma,\n",
        "            pop_size=train_pop_size,\n",
        "            lr=lr,\n",
        "            device=device,\n",
        "            eval_fn=train_eval_fn,\n",
        "            num_sequences=train_num_sequences,\n",
        "            seq_length=seq_length,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        history.append((avg_reward, max_reward))\n",
        "        best_training_reward = max(best_training_reward, max_reward)\n",
        "        print(\n",
        "            f\"[ES-constr Exp {exp_id} | Step {step:02d}] \"\n",
        "            f\"avg={avg_reward:.2f}, max={max_reward:.2f}\"\n",
        "        )\n",
        "\n",
        "    # sample some sequences from final ES-tuned model for diversity analysis\n",
        "    es_final_seqs = []\n",
        "    for _ in range(32):\n",
        "        s = sample_sequence_from_lm_constrained(\n",
        "            lm_model,\n",
        "            alphabet,\n",
        "            base_seq=base_seq,\n",
        "            mutable_mask=mutable_mask,\n",
        "            temperature=temperature,\n",
        "            device=device,\n",
        "        )\n",
        "        es_final_seqs.append(s)\n",
        "\n",
        "    return {\n",
        "        \"exp_id\": exp_id,\n",
        "        \"seed\": seed,\n",
        "        \"history\": history,\n",
        "        \"best_training_reward\": best_training_reward,\n",
        "        \"final_seqs\": es_final_seqs,\n",
        "    }\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 4. SeqEvo multi-objective, constrained\n",
        "##########################################\n",
        "\n",
        "def mutate_sequence_with_mask(seq: str, mut_rate: float, mutable_mask=None) -> str:\n",
        "    aa_list = list(seq)\n",
        "    L = len(aa_list)\n",
        "    if mutable_mask is None:\n",
        "        mutable_mask = [True] * L\n",
        "    assert len(mutable_mask) == L\n",
        "\n",
        "    for i in range(L):\n",
        "        if not mutable_mask[i]:\n",
        "            continue\n",
        "        if random.random() < mut_rate:\n",
        "            aa_list[i] = random.choice(AA_LETTERS)\n",
        "    return \"\".join(aa_list)\n",
        "\n",
        "\n",
        "def sequence_space_evolution_objective_constrained(\n",
        "    exp_id: int,\n",
        "    seed: int,\n",
        "    pop_size: int,\n",
        "    n_generations: int,\n",
        "    n_parents: int,\n",
        "    seq_length: int,\n",
        "    temperature: float,\n",
        "    mut_rate: float,\n",
        "    weight_lm_prior: float,\n",
        "    target_hydro: float,\n",
        "    lambda_hydro: float,\n",
        "    base_seq: str,\n",
        "    mutable_mask,\n",
        "):\n",
        "    \"\"\"\n",
        "    SeqEvo baseline with:\n",
        "      - composite reward (same as ES)\n",
        "      - constrained mutations (only mutable_mask positions can change)\n",
        "      - initial population generated by constrained LM sampling\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== SeqEvo constrained exp {exp_id} (seed={seed}) ===\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    lm_model.load_state_dict(base_state_dict)\n",
        "    lm_model.to(device)\n",
        "    lm_model.eval()\n",
        "\n",
        "    # initial population\n",
        "    population = []\n",
        "    for _ in range(pop_size):\n",
        "        s = sample_sequence_from_lm_constrained(\n",
        "            lm_model,\n",
        "            alphabet,\n",
        "            base_seq=base_seq,\n",
        "            mutable_mask=mutable_mask,\n",
        "            temperature=temperature,\n",
        "            device=device,\n",
        "        )\n",
        "        population.append(s)\n",
        "\n",
        "    best_reward = -1e9\n",
        "    best_plddt = -1e9\n",
        "    best_seq = None\n",
        "    history = []\n",
        "\n",
        "    for gen in range(n_generations):\n",
        "        rewards = []\n",
        "        plddts = []\n",
        "        for seq in population:\n",
        "            r, plddt, lm_logp, norm_lm, h = compute_multiobjective_reward(\n",
        "                seq,\n",
        "                weight_lm_prior=weight_lm_prior,\n",
        "                target_hydro=target_hydro,\n",
        "                lambda_hydro=lambda_hydro,\n",
        "                caller=\"SEQEVO\",\n",
        "            )\n",
        "            rewards.append(r)\n",
        "            plddts.append(plddt)\n",
        "\n",
        "        rewards = np.array(rewards, dtype=np.float32)\n",
        "        plddts = np.array(plddts, dtype=np.float32)\n",
        "\n",
        "        avg_r = float(rewards.mean())\n",
        "        max_r = float(rewards.max())\n",
        "        history.append((avg_r, max_r))\n",
        "\n",
        "        print(\n",
        "            f\"[SeqEvo-constr Exp {exp_id} | Gen {gen:02d}] \"\n",
        "            f\"avg={avg_r:.2f}, max={max_r:.2f}, avg_pLDDT={float(plddts.mean()):.2f}\"\n",
        "        )\n",
        "\n",
        "        idx_best = int(rewards.argmax())\n",
        "        if rewards[idx_best] > best_reward:\n",
        "            best_reward = float(rewards[idx_best])\n",
        "            best_plddt = float(plddts[idx_best])\n",
        "            best_seq = population[idx_best]\n",
        "            print(\n",
        "                f\"[SeqEvo-constr Exp {exp_id}] \"\n",
        "                f\"New best: reward={best_reward:.2f}, pLDDT={best_plddt:.2f}\"\n",
        "            )\n",
        "\n",
        "        if gen == n_generations - 1:\n",
        "            break\n",
        "\n",
        "        parent_indices = rewards.argsort()[::-1][:n_parents]\n",
        "        parents = [population[i] for i in parent_indices]\n",
        "\n",
        "        new_population = [parents[0]]  # elitism\n",
        "        while len(new_population) < pop_size:\n",
        "            parent = random.choice(parents)\n",
        "            child = mutate_sequence_with_mask(parent, mut_rate, mutable_mask)\n",
        "            new_population.append(child)\n",
        "\n",
        "        population = new_population\n",
        "\n",
        "    return {\n",
        "        \"exp_id\": exp_id,\n",
        "        \"seed\": seed,\n",
        "        \"history\": history,\n",
        "        \"best_reward\": best_reward,\n",
        "        \"best_plddt\": best_plddt,\n",
        "        \"best_seq\": best_seq,\n",
        "        \"final_population\": population,\n",
        "    }\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 5. Set up constrained loop regime\n",
        "##########################################\n",
        "\n",
        "# Choose a scaffold from the base LM (fixed seed for reproducibility)\n",
        "random.seed(777)\n",
        "np.random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "SCAFFOLD_LENGTH = 60\n",
        "scaffold_seq = sample_sequence_from_lm(\n",
        "    lm_model,\n",
        "    alphabet,\n",
        "    length=SCAFFOLD_LENGTH,\n",
        "    temperature=1.0,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "# Define central mutable loop: e.g., positions 20..39 (0-based)\n",
        "loop_start = 20\n",
        "loop_end = 40  # exclusive\n",
        "mutable_mask = [False] * SCAFFOLD_LENGTH\n",
        "for i in range(loop_start, loop_end):\n",
        "    mutable_mask[i] = True\n",
        "\n",
        "print(\"\\n=== Constrained loop regime ===\")\n",
        "print(f\"Scaffold sequence (L={SCAFFOLD_LENGTH}):\")\n",
        "print(scaffold_seq)\n",
        "print(f\"Mutable region: positions {loop_start}..{loop_end-1} (0-based)\")\n",
        "print(\"Mutable mask (first 60 positions):\")\n",
        "print(\"\".join(\"X\" if m else \".\" for m in mutable_mask))\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 6. Multi-seed ES vs SeqEvo comparison\n",
        "##########################################\n",
        "\n",
        "N_SEEDS = 3  # bump this up when you have more compute\n",
        "\n",
        "# Match total training ESMFold calls per seed:\n",
        "# ES: steps * pop_size * num_sequences\n",
        "CONSTR_ES_STEPS = 5\n",
        "CONSTR_ES_POP = 4\n",
        "CONSTR_ES_NUM_SEQ = 2\n",
        "ES_BUDGET = CONSTR_ES_STEPS * CONSTR_ES_POP * CONSTR_ES_NUM_SEQ\n",
        "\n",
        "# SeqEvo: generations * population size\n",
        "CONSTR_SEQ_POP = 8\n",
        "CONSTR_SEQ_GENS = 5\n",
        "SEQ_BUDGET = CONSTR_SEQ_POP * CONSTR_SEQ_GENS\n",
        "\n",
        "print(\"\\n=== Constrained multi-seed CONFIG ===\")\n",
        "print(f\"ES budget:    {ES_BUDGET} ESMFold calls (steps={CONSTR_ES_STEPS}, pop={CONSTR_ES_POP}, seq/worker={CONSTR_ES_NUM_SEQ})\")\n",
        "print(f\"SeqEvo budget:{SEQ_BUDGET} ESMFold calls (gens={CONSTR_SEQ_GENS}, pop={CONSTR_SEQ_POP})\")\n",
        "\n",
        "# Composite objective weights\n",
        "WEIGHT_LM_PRIOR = 0.1\n",
        "TARGET_HYDRO = 0.50\n",
        "LAMBDA_HYDRO = 20.0\n",
        "\n",
        "es_results_all = []\n",
        "seqevo_results_all = []\n",
        "es_div_all = []\n",
        "seq_div_all = []\n",
        "\n",
        "reset_esmfold_counters()\n",
        "\n",
        "base_seed = 3000\n",
        "\n",
        "for i in range(N_SEEDS):\n",
        "    seed = base_seed + i\n",
        "\n",
        "    # --- ES ---\n",
        "    es_res = run_es_multiobjective_experiment_constrained(\n",
        "        exp_id=i,\n",
        "        seed=seed,\n",
        "        train_steps=CONSTR_ES_STEPS,\n",
        "        train_pop_size=CONSTR_ES_POP,\n",
        "        train_num_sequences=CONSTR_ES_NUM_SEQ,\n",
        "        seq_length=SCAFFOLD_LENGTH,\n",
        "        temperature=1.0,\n",
        "        sigma=TRAIN_SIGMA,\n",
        "        lr=TRAIN_LR,\n",
        "        weight_lm_prior=WEIGHT_LM_PRIOR,\n",
        "        target_hydro=TARGET_HYDRO,\n",
        "        lambda_hydro=LAMBDA_HYDRO,\n",
        "        base_seq=scaffold_seq,\n",
        "        mutable_mask=mutable_mask,\n",
        "    )\n",
        "    es_results_all.append(es_res)\n",
        "\n",
        "    # diversity of ES samples\n",
        "    es_div = analyze_sequence_set(es_res[\"final_seqs\"], label=f\"ES-exp{i}\")\n",
        "    es_div_all.append(es_div)\n",
        "\n",
        "    # --- SeqEvo ---\n",
        "    seq_res = sequence_space_evolution_objective_constrained(\n",
        "        exp_id=i,\n",
        "        seed=seed + 10_000,\n",
        "        pop_size=CONSTR_SEQ_POP,\n",
        "        n_generations=CONSTR_SEQ_GENS,\n",
        "        n_parents=max(4, CONSTR_SEQ_POP // 4),\n",
        "        seq_length=SCAFFOLD_LENGTH,\n",
        "        temperature=1.0,\n",
        "        mut_rate=0.05,\n",
        "        weight_lm_prior=WEIGHT_LM_PRIOR,\n",
        "        target_hydro=TARGET_HYDRO,\n",
        "        lambda_hydro=LAMBDA_HYDRO,\n",
        "        base_seq=scaffold_seq,\n",
        "        mutable_mask=mutable_mask,\n",
        "    )\n",
        "    seqevo_results_all.append(seq_res)\n",
        "\n",
        "    seq_div = analyze_sequence_set(seq_res[\"final_population\"], label=f\"SeqEvo-exp{i}\")\n",
        "    seq_div_all.append(seq_div)\n",
        "\n",
        "print(\"\\n=== Training ESMFold compute summary (constrained runs) ===\")\n",
        "print(f\"ES ESMFold calls used:      {ESMFOLD_CALLS_ES}\")\n",
        "print(f\"SeqEvo ESMFold calls used:  {ESMFOLD_CALLS_SEQEVO}\")\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 7. Aggregate stats across seeds\n",
        "##########################################\n",
        "\n",
        "def summarize_best_rewards(es_results_all, seqevo_results_all):\n",
        "    es_best = [r[\"best_training_reward\"] for r in es_results_all]\n",
        "    se_best = [r[\"best_reward\"] for r in seqevo_results_all]\n",
        "\n",
        "    print(\"\\n=== Best training reward across seeds ===\")\n",
        "    print(f\"ES:    mean={np.mean(es_best):.2f} ± {np.std(es_best):.2f}\")\n",
        "    print(f\"SeqEV: mean={np.mean(se_best):.2f} ± {np.std(se_best):.2f}\")\n",
        "    return es_best, se_best\n",
        "\n",
        "\n",
        "def summarize_diversity(div_list, label):\n",
        "    ids = [d[\"mean_identity\"] for d in div_list if not math.isnan(d[\"mean_identity\"])]\n",
        "    ents = [d[\"mean_entropy\"] for d in div_list]\n",
        "    print(f\"\\n=== Diversity summary: {label} ===\")\n",
        "    print(f\"Mean pairwise identity: {np.mean(ids):.3f} ± {np.std(ids):.3f}\")\n",
        "    print(f\"Mean per-pos entropy:   {np.mean(ents):.3f} ± {np.std(ents):.3f}\")\n",
        "    return ids, ents\n",
        "\n",
        "\n",
        "es_best, se_best = summarize_best_rewards(es_results_all, seqevo_results_all)\n",
        "es_ids, es_ents = summarize_diversity(es_div_all, \"ES\")\n",
        "se_ids, se_ents = summarize_diversity(seq_div_all, \"SeqEvo\")\n",
        "\n",
        "\n",
        "##########################################\n",
        "# 8. Simple bar plots: reward vs diversity\n",
        "##########################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(N_SEEDS)\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(x - 0.15, es_best, width=0.3, label=\"ES best reward\")\n",
        "plt.bar(x + 0.15, se_best, width=0.3, label=\"SeqEvo best reward\")\n",
        "plt.xlabel(\"Seed\")\n",
        "plt.ylabel(\"Best training reward\")\n",
        "plt.title(\"Best composite reward per seed (constrained loop regime)\")\n",
        "plt.legend()\n",
        "plt.grid(True, axis=\"y\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar([0-0.15, 0+0.15],\n",
        "        [np.mean(es_ids), np.mean(se_ids)],\n",
        "        yerr=[np.std(es_ids), np.std(se_ids)],\n",
        "        width=0.3,\n",
        "        tick_label=[\"ES\", \"SeqEvo\"])\n",
        "plt.ylabel(\"Mean pairwise identity\")\n",
        "plt.title(\"Diversity: mean pairwise identity across seeds\")\n",
        "plt.grid(True, axis=\"y\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar([0-0.15, 0+0.15],\n",
        "        [np.mean(es_ents), np.mean(se_ents)],\n",
        "        yerr=[np.std(es_ents), np.std(se_ents)],\n",
        "        width=0.3,\n",
        "        tick_label=[\"ES\", \"SeqEvo\"])\n",
        "plt.ylabel(\"Mean per-position entropy (bits)\")\n",
        "plt.title(\"Diversity: mean entropy across seeds\")\n",
        "plt.grid(True, axis=\"y\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BBMpo8TqJwcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Publication-oriented constrained loop benchmark:\n",
        "ES (EGGROLL-style) vs sequence-space evolution, multi-seed / multi-loop,\n",
        "with a stronger multi-objective reward.\n",
        "\n",
        "This cell depends on:\n",
        "- lm_model, alphabet, fold_model, device\n",
        "- AA_LETTERS, cls_idx, eos_idx\n",
        "- evaluate_sequence_with_esmfold(sequence) -> mean_pLDDT\n",
        "- egg_ctx, linear_weights, es_step_eggroll (EGGROLL-style ES)\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 0. GLOBAL CONFIG\n",
        "# -----------------------------\n",
        "\n",
        "# Two loop regimes on the same scaffold (you can add more)\n",
        "SCAFFOLD_SEQ = \"KSGISCHGGIWIASFGKHKKRCKAKYERQYVRLIYKNKDKKFSTIKGLWKMIEAEYPDKI\"\n",
        "assert len(SCAFFOLD_SEQ) == 60\n",
        "\n",
        "CONSTRAINED_SETTINGS = [\n",
        "    {\n",
        "        \"name\": \"loop20_39\",\n",
        "        \"scaffold\": SCAFFOLD_SEQ,\n",
        "        \"mut_start\": 20,   # inclusive\n",
        "        \"mut_end\": 40,     # exclusive\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"loop10_29\",\n",
        "        \"scaffold\": SCAFFOLD_SEQ,\n",
        "        \"mut_start\": 10,\n",
        "        \"mut_end\": 30,\n",
        "    },\n",
        "]\n",
        "\n",
        "# Multi-objective weights\n",
        "LM_NLL_SCALE = 10.0        # penalty on LM negative log-likelihood per residue\n",
        "HYDRO_TARGET = 0.45        # target hydrophobic fraction in the loop\n",
        "HYDRO_SCALE = 80.0\n",
        "POS_CHARGE_TARGET = 0.35   # K,R,H fraction\n",
        "NEG_CHARGE_TARGET = 0.35   # D,E fraction\n",
        "CHARGE_SCALE = 60.0\n",
        "\n",
        "# ES config (per experiment)\n",
        "ES_TOTAL_EVALS = 40        # total ESMFold evals per method per seed\n",
        "ES_POP_SIZE = 4\n",
        "ES_NUM_SEQ_PER_MEMBER = 2\n",
        "ES_NUM_STEPS = ES_TOTAL_EVALS // (ES_POP_SIZE * ES_NUM_SEQ_PER_MEMBER)\n",
        "ES_RANK = 4\n",
        "ES_SIGMA = 0.02\n",
        "ES_LR = 0.03\n",
        "ES_TEMP = 1.0\n",
        "\n",
        "# Sequence evolution config (per experiment)\n",
        "SEQEVO_TOTAL_EVALS = ES_TOTAL_EVALS  # match ES\n",
        "SEQEVO_POP_SIZE = 8\n",
        "SEQEVO_N_GENERATIONS = SEQEVO_TOTAL_EVALS // SEQEVO_POP_SIZE\n",
        "SEQEVO_MUT_RATE = 0.05  # per-position in loop\n",
        "\n",
        "# Which Linear weights to train in ES (subset of `linear_weights`)\n",
        "NUM_LINEAR_TO_TRAIN = 24\n",
        "\n",
        "# Seeds and settings\n",
        "N_SEEDS = 3\n",
        "BASE_SEED = 4000\n",
        "\n",
        "print(\"=== Constrained benchmark CONFIG ===\")\n",
        "print(f\"SCAFFOLD_SEQ (L={len(SCAFFOLD_SEQ)}): {SCAFFOLD_SEQ}\")\n",
        "for cfg in CONSTRAINED_SETTINGS:\n",
        "    print(\n",
        "        f\"  Setting {cfg['name']}: loop {cfg['mut_start']}..{cfg['mut_end']-1} \"\n",
        "        f\"(length={cfg['mut_end']-cfg['mut_start']})\"\n",
        "    )\n",
        "print(f\"ES:     evals={ES_TOTAL_EVALS}, steps={ES_NUM_STEPS}, pop={ES_POP_SIZE}, seq/worker={ES_NUM_SEQ_PER_MEMBER}\")\n",
        "print(f\"SeqEvo: evals={SEQEVO_TOTAL_EVALS}, gens={SEQEVO_N_GENERATIONS}, pop={SEQEVO_POP_SIZE}\")\n",
        "print(f\"ES trains last {NUM_LINEAR_TO_TRAIN} Linear weights out of {len(linear_weights)} total.\")\n",
        "\n",
        "# Subset of Linear weights that ES will perturb\n",
        "trained_linear_weights_pub = linear_weights[-NUM_LINEAR_TO_TRAIN:]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1. BASIC UTILITIES\n",
        "# -----------------------------\n",
        "\n",
        "AA_SET = set(AA_LETTERS)\n",
        "HYDROPHOBIC = set(\"AILMFWYV\")\n",
        "POSITIVE = set(\"KRH\")\n",
        "NEGATIVE = set(\"DE\")\n",
        "\n",
        "\n",
        "def enforce_scaffold_constraint(scaffold: str, loop_seq: str, mut_start: int, mut_end: int) -> str:\n",
        "    \"\"\"Return full sequence = scaffold outside [mut_start:mut_end], loop_seq inside.\"\"\"\n",
        "    assert len(scaffold) == len(SCAFFOLD_SEQ), \"All scaffolds assumed same length here.\"\n",
        "    assert mut_end > mut_start\n",
        "    assert len(loop_seq) == (mut_end - mut_start)\n",
        "    return scaffold[:mut_start] + loop_seq + scaffold[mut_start + len(loop_seq):]\n",
        "\n",
        "\n",
        "def random_loop_from_lm(loop_len: int, temperature: float = 1.0) -> str:\n",
        "    \"\"\"Sample a loop of given length from lm_model (independent of scaffold).\"\"\"\n",
        "    from torch.nn import functional as F\n",
        "\n",
        "    tokens = torch.full(\n",
        "        (1, loop_len + 2),\n",
        "        fill_value=alphabet.mask_idx,\n",
        "        device=device,\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "    tokens[0, 0] = cls_idx\n",
        "    tokens[0, -1] = eos_idx\n",
        "\n",
        "    for pos in range(1, loop_len + 1):\n",
        "        out = lm_model(tokens, repr_layers=[], return_contacts=False)\n",
        "        logits = out[\"logits\"][0, pos]\n",
        "        aa_logits = logits[AA_INDICES]\n",
        "        probs = F.softmax(aa_logits / temperature, dim=-1)\n",
        "        aa_idx = torch.multinomial(probs, num_samples=1)\n",
        "        tok_id = AA_INDICES[aa_idx]\n",
        "        tokens[0, pos] = tok_id\n",
        "\n",
        "    seq_tokens = tokens[0, 1:-1].tolist()\n",
        "    seq = \"\".join(alphabet.get_tok(t) for t in seq_tokens)\n",
        "    return seq\n",
        "\n",
        "\n",
        "def mutate_loop(seq: str, mut_start: int, mut_end: int, mut_rate: float) -> str:\n",
        "    \"\"\"Randomly mutate positions within [mut_start:mut_end) with probability mut_rate.\"\"\"\n",
        "    L = len(seq)\n",
        "    loop_len = mut_end - mut_start\n",
        "    seq_list = list(seq)\n",
        "    for i in range(loop_len):\n",
        "        global_pos = mut_start + i\n",
        "        if random.random() < mut_rate:\n",
        "            orig = seq_list[global_pos]\n",
        "            choices = [aa for aa in AA_LETTERS if aa != orig]\n",
        "            seq_list[global_pos] = random.choice(choices)\n",
        "    return \"\".join(seq_list)\n",
        "\n",
        "\n",
        "def sequence_identity(s1: str, s2: str) -> float:\n",
        "    assert len(s1) == len(s2)\n",
        "    return sum(a == b for a, b in zip(s1, s2)) / len(s1)\n",
        "\n",
        "\n",
        "def mean_pairwise_identity(seqs):\n",
        "    if len(seqs) < 2:\n",
        "        return 1.0\n",
        "    ids = []\n",
        "    for i in range(len(seqs)):\n",
        "        for j in range(i + 1, len(seqs)):\n",
        "            ids.append(sequence_identity(seqs[i], seqs[j]))\n",
        "    return float(np.mean(ids))\n",
        "\n",
        "\n",
        "def mean_position_entropy(seqs):\n",
        "    \"\"\"Average Shannon entropy (bits) over positions for the set of sequences.\"\"\"\n",
        "    if len(seqs) == 0:\n",
        "        return 0.0\n",
        "    L = len(seqs[0])\n",
        "    for s in seqs:\n",
        "        assert len(s) == L\n",
        "    entropies = []\n",
        "    for pos in range(L):\n",
        "        counts = defaultdict(int)\n",
        "        for s in seqs:\n",
        "            counts[s[pos]] += 1\n",
        "        freqs = np.array([c / len(seqs) for c in counts.values()], dtype=float)\n",
        "        H = -(freqs * np.log2(freqs + 1e-12)).sum()\n",
        "        entropies.append(H)\n",
        "    return float(np.mean(entropies))\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2. LM PRIOR & LOOP PENALTIES\n",
        "# -----------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def lm_sequence_avg_nll(seq: str) -> float:\n",
        "    \"\"\"\n",
        "    Approximate negative log-likelihood per residue using ESM-2.\n",
        "    We treat the logits of the unmasked forward as pseudo-conditional probabilities.\n",
        "    \"\"\"\n",
        "    token_ids = [alphabet.get_idx(a) for a in seq]\n",
        "    tokens = torch.tensor(\n",
        "        [[cls_idx] + token_ids + [eos_idx]],\n",
        "        device=device,\n",
        "        dtype=torch.long,\n",
        "    )\n",
        "    out = lm_model(tokens, repr_layers=[], return_contacts=False)\n",
        "    logits = out[\"logits\"][0, 1:-1]  # (L, vocab)\n",
        "    log_probs = torch.log_softmax(logits, dim=-1)\n",
        "    idxs = torch.tensor(token_ids, device=device, dtype=torch.long)\n",
        "    token_logp = log_probs[torch.arange(len(seq), device=device), idxs]\n",
        "    avg_nll = float(-token_logp.mean().item())\n",
        "    return avg_nll\n",
        "\n",
        "\n",
        "def loop_biophysics_penalty(seq: str, mut_start: int, mut_end: int):\n",
        "    loop = seq[mut_start:mut_end]\n",
        "    L = len(loop)\n",
        "    hyd = sum(aa in HYDROPHOBIC for aa in loop) / L\n",
        "    pos = sum(aa in POSITIVE for aa in loop) / L\n",
        "    neg = sum(aa in NEGATIVE for aa in loop) / L\n",
        "\n",
        "    hyd_pen = HYDRO_SCALE * max(0.0, hyd - HYDRO_TARGET) ** 2\n",
        "    pos_pen = CHARGE_SCALE * max(0.0, pos - POS_CHARGE_TARGET) ** 2\n",
        "    neg_pen = CHARGE_SCALE * max(0.0, neg - NEG_CHARGE_TARGET) ** 2\n",
        "    total_pen = hyd_pen + pos_pen + neg_pen\n",
        "    comps = {\n",
        "        \"hydrophobic_frac\": hyd,\n",
        "        \"pos_frac\": pos,\n",
        "        \"neg_frac\": neg,\n",
        "        \"hyd_pen\": hyd_pen,\n",
        "        \"pos_pen\": pos_pen,\n",
        "        \"neg_pen\": neg_pen,\n",
        "    }\n",
        "    return total_pen, comps\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3. MULTI-OBJECTIVE REWARD\n",
        "# -----------------------------\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_sequence_multiobjective(\n",
        "    full_seq: str,\n",
        "    mut_start: int,\n",
        "    mut_end: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Multi-objective reward used in the constrained benchmark.\n",
        "    Components:\n",
        "      - + mean pLDDT (ESMFold)\n",
        "      - - LM_NLL_SCALE * avg NLL (ESM-2 pseudo-likelihood)\n",
        "      - - loop biophysics penalty (hydrophobicity / charge)\n",
        "    \"\"\"\n",
        "    # Structural confidence\n",
        "    mean_plddt = evaluate_sequence_with_esmfold(full_seq)\n",
        "\n",
        "    # LM prior\n",
        "    avg_nll = lm_sequence_avg_nll(full_seq)\n",
        "    lm_penalty = LM_NLL_SCALE * avg_nll\n",
        "\n",
        "    # Simple biophysics\n",
        "    loop_penalty, loop_comps = loop_biophysics_penalty(full_seq, mut_start, mut_end)\n",
        "\n",
        "    reward = mean_plddt - lm_penalty - loop_penalty\n",
        "\n",
        "    comps = {\n",
        "        \"plddt\": mean_plddt,\n",
        "        \"avg_nll\": avg_nll,\n",
        "        \"lm_penalty\": lm_penalty,\n",
        "        \"loop_penalty\": loop_penalty,\n",
        "    }\n",
        "    comps.update(loop_comps)\n",
        "    return reward, comps\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4. ES EXPERIMENT (per setting, per seed)\n",
        "# -----------------------------\n",
        "\n",
        "def es_constrained_eval_once(\n",
        "    scaffold_cfg,\n",
        "    num_sequences: int,\n",
        "    temperature: float,\n",
        "):\n",
        "    \"\"\"\n",
        "    For a given ES parameter vector (implicitly through egg_ctx),\n",
        "    sample num_sequences loops from lm_model, enforce scaffold,\n",
        "    and return mean composite reward.\n",
        "    \"\"\"\n",
        "    mut_start = scaffold_cfg[\"mut_start\"]\n",
        "    mut_end = scaffold_cfg[\"mut_end\"]\n",
        "    loop_len = mut_end - mut_start\n",
        "    scaffold = scaffold_cfg[\"scaffold\"]\n",
        "\n",
        "    rewards = []\n",
        "    for _ in range(num_sequences):\n",
        "        loop_seq = random_loop_from_lm(loop_len, temperature=temperature)\n",
        "        full_seq = enforce_scaffold_constraint(scaffold, loop_seq, mut_start, mut_end)\n",
        "        r, _ = evaluate_sequence_multiobjective(full_seq, mut_start, mut_end)\n",
        "        rewards.append(r)\n",
        "    return float(np.mean(rewards)), rewards\n",
        "\n",
        "\n",
        "def run_es_constrained_single(\n",
        "    scaffold_cfg,\n",
        "    seed: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Run one ES experiment for a single scaffold+loop setting.\n",
        "    Returns dict with:\n",
        "      - 'history' (avg, max per step)\n",
        "      - 'best_reward'\n",
        "      - 'best_sequences'\n",
        "      - 'final_samples' (for diversity)\n",
        "    \"\"\"\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\"ES constrained run: setting={scaffold_cfg['name']} seed={seed}\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    history = []\n",
        "    best_reward = -1e9\n",
        "    best_sequences = []\n",
        "\n",
        "    def eval_fn(num_sequences, seq_length_unused, temperature):\n",
        "        avg_r, rewards = es_constrained_eval_once(\n",
        "            scaffold_cfg,\n",
        "            num_sequences=num_sequences,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return avg_r, rewards\n",
        "\n",
        "    # ES steps\n",
        "    for step in range(1, ES_NUM_STEPS + 1):\n",
        "        avg_r, max_r, all_r = es_step_eggroll(\n",
        "            lm_model,\n",
        "            trained_linear_weights_pub,\n",
        "            rank=ES_RANK,\n",
        "            sigma=ES_SIGMA,\n",
        "            pop_size=ES_POP_SIZE,\n",
        "            lr=ES_LR,\n",
        "            device=device,\n",
        "            eval_fn=eval_fn,\n",
        "            num_sequences=ES_NUM_SEQ_PER_MEMBER,\n",
        "            seq_length=len(scaffold_cfg[\"scaffold\"]),\n",
        "            temperature=ES_TEMP,\n",
        "        )\n",
        "        history.append((avg_r, max_r))\n",
        "        print(f\"[ES {scaffold_cfg['name']} | step {step:02d}] avg={avg_r:.2f}, max={max_r:.2f}\")\n",
        "\n",
        "        if max_r > best_reward:\n",
        "            best_reward = max_r\n",
        "\n",
        "    # Sample a batch from final ES-updated model for diversity analysis\n",
        "    mut_start = scaffold_cfg[\"mut_start\"]\n",
        "    mut_end = scaffold_cfg[\"mut_end\"]\n",
        "    loop_len = mut_end - mut_start\n",
        "    scaffold = scaffold_cfg[\"scaffold\"]\n",
        "    final_samples = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(ES_POP_SIZE * ES_NUM_SEQ_PER_MEMBER * 2):\n",
        "            loop_seq = random_loop_from_lm(loop_len, temperature=ES_TEMP)\n",
        "            full_seq = enforce_scaffold_constraint(scaffold, loop_seq, mut_start, mut_end)\n",
        "            final_samples.append(full_seq)\n",
        "\n",
        "    div_identity = mean_pairwise_identity(final_samples)\n",
        "    div_entropy = mean_position_entropy(final_samples)\n",
        "    print(\n",
        "        f\"[ES diversity] n={len(final_samples)}, \"\n",
        "        f\"mean identity={div_identity:.3f}, mean entropy={div_entropy:.3f}\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"history\": history,\n",
        "        \"best_reward\": best_reward,\n",
        "        \"final_samples\": final_samples,\n",
        "        \"div_identity\": div_identity,\n",
        "        \"div_entropy\": div_entropy,\n",
        "    }\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5. SEQUENCE EVOLUTION EXPERIMENT\n",
        "# -----------------------------\n",
        "\n",
        "def run_seqevo_constrained_single(\n",
        "    scaffold_cfg,\n",
        "    seed: int,\n",
        "):\n",
        "    \"\"\"\n",
        "    Simple sequence-space evolution baseline under same scaffold+loop constraint.\n",
        "    Mutations restricted to the loop.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== SeqEvo constrained run: setting={} seed={} ===\".format(scaffold_cfg[\"name\"], seed))\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    mut_start = scaffold_cfg[\"mut_start\"]\n",
        "    mut_end = scaffold_cfg[\"mut_end\"]\n",
        "    loop_len = mut_end - mut_start\n",
        "    scaffold = scaffold_cfg[\"scaffold\"]\n",
        "\n",
        "    # Initialize population by sampling random loops from LM\n",
        "    population = []\n",
        "    for _ in range(SEQEVO_POP_SIZE):\n",
        "        loop_seq = random_loop_from_lm(loop_len, temperature=ES_TEMP)\n",
        "        full_seq = enforce_scaffold_constraint(scaffold, loop_seq, mut_start, mut_end)\n",
        "        population.append(full_seq)\n",
        "\n",
        "    history = []\n",
        "    best_reward = -1e9\n",
        "    best_seq = None\n",
        "\n",
        "    for gen in range(SEQEVO_N_GENERATIONS):\n",
        "        rewards = []\n",
        "        pl_ddts = []\n",
        "        for seq in population:\n",
        "            r, comps = evaluate_sequence_multiobjective(seq, mut_start, mut_end)\n",
        "            rewards.append(r)\n",
        "            pl_ddts.append(comps[\"plddt\"])\n",
        "        rewards = np.array(rewards, dtype=float)\n",
        "        history.append((float(rewards.mean()), float(rewards.max())))\n",
        "        print(\n",
        "            f\"[SeqEvo {scaffold_cfg['name']} | gen {gen:02d}] \"\n",
        "            f\"avg={rewards.mean():.2f}, max={rewards.max():.2f}, avg_pLDDT={np.mean(pl_ddts):.2f}\"\n",
        "        )\n",
        "\n",
        "        # Track best\n",
        "        max_idx = int(rewards.argmax())\n",
        "        if rewards[max_idx] > best_reward:\n",
        "            best_reward = float(rewards[max_idx])\n",
        "            best_seq = population[max_idx]\n",
        "\n",
        "        # Selection + mutation\n",
        "        parent_indices = rewards.argsort()[::-1][: max(2, SEQEVO_POP_SIZE // 4)]\n",
        "        parents = [population[i] for i in parent_indices]\n",
        "\n",
        "        new_population = []\n",
        "        while len(new_population) < SEQEVO_POP_SIZE:\n",
        "            parent = random.choice(parents)\n",
        "            child = mutate_loop(parent, mut_start, mut_end, SEQEVO_MUT_RATE)\n",
        "            new_population.append(child)\n",
        "        population = new_population\n",
        "\n",
        "    # Diversity on final population\n",
        "    div_identity = mean_pairwise_identity(population)\n",
        "    div_entropy = mean_position_entropy(population)\n",
        "    print(\n",
        "        f\"[SeqEvo diversity] n={len(population)}, \"\n",
        "        f\"mean identity={div_identity:.3f}, mean entropy={div_entropy:.3f}\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"history\": history,\n",
        "        \"best_reward\": best_reward,\n",
        "        \"best_seq\": best_seq,\n",
        "        \"final_population\": population,\n",
        "        \"div_identity\": div_identity,\n",
        "        \"div_entropy\": div_entropy,\n",
        "    }\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6. FULL BENCHMARK OVER LOOPS & SEEDS\n",
        "# -----------------------------\n",
        "\n",
        "def run_full_constrained_benchmark():\n",
        "    \"\"\"\n",
        "    Run ES and SeqEvo for each constrained setting and seed.\n",
        "    Aggregate performance and diversity.\n",
        "    \"\"\"\n",
        "    all_results = {\n",
        "        \"ES\": defaultdict(list),      # setting -> list of per-seed dicts\n",
        "        \"SeqEvo\": defaultdict(list),\n",
        "    }\n",
        "\n",
        "    for cfg in CONSTRAINED_SETTINGS:\n",
        "        for s in range(N_SEEDS):\n",
        "            es_seed = BASE_SEED + s\n",
        "            seq_seed = BASE_SEED + 1000 + s\n",
        "\n",
        "            es_res = run_es_constrained_single(cfg, es_seed)\n",
        "            se_res = run_seqevo_constrained_single(cfg, seq_seed)\n",
        "\n",
        "            all_results[\"ES\"][cfg[\"name\"]].append(es_res)\n",
        "            all_results[\"SeqEvo\"][cfg[\"name\"]].append(se_res)\n",
        "\n",
        "    # Aggregate statistics and plot\n",
        "    for cfg in CONSTRAINED_SETTINGS:\n",
        "        name = cfg[\"name\"]\n",
        "        es_runs = all_results[\"ES\"][name]\n",
        "        se_runs = all_results[\"SeqEvo\"][name]\n",
        "\n",
        "        es_best = np.array([r[\"best_reward\"] for r in es_runs], dtype=float)\n",
        "        se_best = np.array([r[\"best_reward\"] for r in se_runs], dtype=float)\n",
        "\n",
        "        es_id = np.array([r[\"div_identity\"] for r in es_runs], dtype=float)\n",
        "        se_id = np.array([r[\"div_identity\"] for r in se_runs], dtype=float)\n",
        "        es_ent = np.array([r[\"div_entropy\"] for r in es_runs], dtype=float)\n",
        "        se_ent = np.array([r[\"div_entropy\"] for r in se_runs], dtype=float)\n",
        "\n",
        "        print(\"\\n=== Summary for setting:\", name, \"===\")\n",
        "        print(\n",
        "            f\"Best reward ES:     {es_best.mean():.2f} ± {es_best.std():.2f} \"\n",
        "            f\"(n={len(es_best)})\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Best reward SeqEvo: {se_best.mean():.2f} ± {se_best.std():.2f} \"\n",
        "            f\"(n={len(se_best)})\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Diversity (identity) ES: {es_id.mean():.3f} ± {es_id.std():.3f}; \"\n",
        "            f\"SeqEvo: {se_id.mean():.3f} ± {se_id.std():.3f}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Diversity (entropy)  ES: {es_ent.mean():.3f} ± {es_ent.std():.3f}; \"\n",
        "            f\"SeqEvo: {se_ent.mean():.3f} ± {se_ent.std():.3f}\"\n",
        "        )\n",
        "\n",
        "        # Plot best reward per seed\n",
        "        seeds = np.arange(len(es_best))\n",
        "        width = 0.35\n",
        "        plt.figure(figsize=(5, 3.5))\n",
        "        plt.bar(seeds - width / 2, es_best, width, label=\"ES best\")\n",
        "        plt.bar(seeds + width / 2, se_best, width, label=\"SeqEvo best\")\n",
        "        plt.xlabel(\"Seed\")\n",
        "        plt.ylabel(\"Best composite reward\")\n",
        "        plt.title(f\"Best reward per seed ({name})\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Diversity plots\n",
        "        plt.figure(figsize=(4, 3.5))\n",
        "        plt.bar([0, 1], [es_id.mean(), se_id.mean()], yerr=[es_id.std(), se_id.std()])\n",
        "        plt.xticks([0, 1], [\"ES\", \"SeqEvo\"])\n",
        "        plt.ylabel(\"Mean pairwise identity\")\n",
        "        plt.title(f\"Diversity: identity ({name})\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure(figsize=(4, 3.5))\n",
        "        plt.bar([0, 1], [es_ent.mean(), se_ent.mean()], yerr=[es_ent.std(), se_ent.std()])\n",
        "        plt.xticks([0, 1], [\"ES\", \"SeqEvo\"])\n",
        "        plt.ylabel(\"Mean per-position entropy (bits)\")\n",
        "        plt.title(f\"Diversity: entropy ({name})\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 7. RUN THE BENCHMARK\n",
        "# -----------------------------\n",
        "\n",
        "full_results = run_full_constrained_benchmark()\n",
        "print(\"\\nBenchmark finished.\")"
      ],
      "metadata": {
        "id": "nWh_dH3IOK2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "89O676wniZzR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}